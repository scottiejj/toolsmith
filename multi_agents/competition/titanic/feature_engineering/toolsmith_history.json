[
    {
        "role": "user",
        "content": "You are an excellent toolsmith.\nYou design phase-appropriate tools and their documentation for Planner/Developer to use."
    },
    {
        "role": "user",
        "content": "# TASK\n\nPlease design ML tools for the current development phase: Feature Engineering.\nYour tools should include BOTH:\n1. Dataset-specific tools: Tailored to the unique characteristics of this competition's data\n2. Commonly reusable tools: General-purpose utilities applicable across data science tasks\n\nThe Planner and Developer will use these tools to execute tasks in this phase.\nI will provide you with DATASET BACKGROUND, PHASE CONTEXT, and previous reports and plans.\n\nYou can use the following reasoning pattern to design the tools:\n1. Understand the dataset domain and phase objectives.\n2. Identify dataset-specific needs (e.g., domain-specific transformations, competition-specific patterns)\n3. Identify commonly needed operations for this phase type (e.g., standard EDA plots, generic cleaners)\n4. For each tool, ask yourself and answer:\n    - \"Is this tool dataset-specific or generally reusable?\"\n    - \"What parameters should this tool accept to be flexible?\"\n    - \"Which data types (numerical, categorical, datetime, ID) does this tool handle?\"\n    - \"What are the expected inputs and outputs?\"\n    - \"What constraints or edge cases must the tool handle?\"\n5. Ensure tools are:\n    - Phase-appropriate (EDA tools differ from cleaning or feature engineering tools)\n    - Well-balanced (mix of specific + general utilities)\n    - Well-documented (clear parameter descriptions, applicable situations, and usage notes)\n    - Compliant with README format (JSON schema + Markdown documentation)\n\nDesign as many tools as needed to comprehensively support this phase, but prioritize quality over quantity.\nFocus on tools that will be most useful to the Planner and Developer.\n\n\n# CONSTRAINTS\n\n## CONSTRAINTS ##\n1. Tool Balance:\n   - Design tools appropriate for phase: Feature Engineering\n   - Include BOTH dataset-specific AND commonly reusable tools\n   - Avoid duplicating existing tools (list provided)\n   - Prioritize quality and usefulness over quantity\n   \n2. Technical Requirements:\n   - Use only: pandas, numpy, scikit-learn, scipy, matplotlib, seaborn\n   - No file I/O operations inside tools\n   - No network calls or external dependencies\n   - All tools must be pure functions (no global state modifications)\n\n3. Design Principles:\n   - Parameterize column names (never hard-code feature names)\n   - Provide clear, actionable error messages\n   - Handle edge cases gracefully (empty DataFrames, missing columns, wrong types)\n   - Include type hints for all parameters and return values\n   - Document assumptions and limitations in notes\n   - Mark in description whether tool is \"dataset-specific\" or \"reusable\"\n\n4. Output Requirements:\n   - Each tool MUST have corresponding JSON schema entry\n   - Each tool MUST have markdown documentation\n   - Follow exact README format for both JSON and Markdown\n   - Assign correct category: \"data_cleaning\" | \"feature_engineering\" | \"model_build_predict\"\n\n\n# GUIDELINES\n\n## PHASE-SPECIFIC TOOL GUIDELINES ##\n\n### Preliminary EDA Phase\n- Focus on: exploratory visualization helpers, summary statistic functions, correlation analysis utilities\n- Example tools: multi-feature distribution plotter, automated outlier detector, target-feature relationship analyzer\n\n### Data Cleaning Phase\n- Focus on: missing value handlers, outlier treatment, data type converters, inconsistency fixers\n- Example tools: intelligent missing value imputer (context-aware), categorical encoder with rare category handling, datetime parser with error recovery\n\n### In-depth EDA Phase\n- Focus on: advanced statistical analysis, interaction detection, segmentation helpers\n- Example tools: feature interaction heatmap generator, target stratified analysis, statistical test automation\n\n### Feature Engineering Phase\n- Focus on: transformation utilities, aggregation helpers, domain-specific feature creators\n- Example tools: polynomial/interaction feature generator with auto-selection, time-based feature extractor, target encoding with regularization\n\n### Model Building Phase\n- Focus on: training helpers, evaluation utilities, hyperparameter search wrappers\n- Example tools: cross-validation scorer with multiple metrics, model ensemble builder, prediction calibrator\n\n\n\n## TASK: PRODUCE A COMPLETE PYTHON MODULE OF TOOLS ##\n\nYou are designing tools for the current phase: Feature Engineering.\nUsing the dataset background provided, generate a SINGLE Python module that contains multiple functions.\n\nStrict rules:\n- Follow the style of functions in `ml_tools.py` (clear docstrings: Args, Returns, Raises, Examples when helpful)\n- Use only pandas, numpy, scikit-learn, scipy.stats (standard library allowed); no file/network I/O\n- Pure functions: do not mutate global state; return results explicitly\n- Parameterize column names; no hard-coded feature names\n- Validate inputs and raise ValueError/TypeError with actionable messages\n- Type-hint all parameters and return values\n- Consolidate imports at the top; avoid duplicates\n\nOutput format:\n- Return EXACTLY one fenced Python block containing the entire module content\n- Start with imports, then define all functions\n- One function per tool, snake_case names\n\nModule composition guidance:\n- Include BOTH dataset-specific and reusable tools for the phase\n- Focus on high-utility, phase-appropriate tools\n- Prefer 6\u201312 tools rather than many superficial ones\n\nContext:\nPhase: Feature Engineering\nCompetition: titanic\nDataset Background:\n\n"
    },
    {
        "role": "assistant",
        "content": "```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom scipy.stats import zscore\nfrom typing import List, Union, Dict, Any, Optional\n\ndef create_polynomial_features(df: pd.DataFrame, feature_cols: List[str], degree: int = 2) -> pd.DataFrame:\n    \"\"\"\n    Generate polynomial features from the specified feature columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        feature_cols (List[str]): List of column names to generate polynomial features from.\n        degree (int): The degree of the polynomial features.\n\n    Returns:\n        pd.DataFrame: DataFrame with polynomial features added.\n\n    Raises:\n        ValueError: If feature_cols are not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n        >>> create_polynomial_features(df, ['A', 'B'], degree=2)\n    \"\"\"\n    if not all(col in df.columns for col in feature_cols):\n        raise ValueError(f\"One or more columns in {feature_cols} are not in the DataFrame.\")\n    \n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    poly_features = poly.fit_transform(df[feature_cols])\n    poly_feature_names = poly.get_feature_names_out(feature_cols)\n    \n    return pd.DataFrame(poly_features, columns=poly_feature_names, index=df.index)\n\ndef one_hot_encode(df: pd.DataFrame, categorical_cols: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Perform one-hot encoding on specified categorical columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        categorical_cols (List[str]): List of column names to one-hot encode.\n\n    Returns:\n        pd.DataFrame: DataFrame with one-hot encoded columns added.\n\n    Raises:\n        ValueError: If categorical_cols are not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'Color': ['Red', 'Green'], 'Size': ['S', 'M']})\n        >>> one_hot_encode(df, ['Color', 'Size'])\n    \"\"\"\n    if not all(col in df.columns for col in categorical_cols):\n        raise ValueError(f\"One or more columns in {categorical_cols} are not in the DataFrame.\")\n    \n    encoder = OneHotEncoder(sparse=False, drop='first')\n    encoded_features = encoder.fit_transform(df[categorical_cols])\n    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols), index=df.index)\n    \n    return pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n\ndef fill_missing_values(df: pd.DataFrame, strategy: str = 'mean', columns: Optional[List[str]] = None) -> pd.DataFrame:\n    \"\"\"\n    Fill missing values in numerical columns using specified strategy.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        strategy (str): Strategy to use for filling missing values ('mean', 'median', or 'most_frequent').\n        columns (Optional[List[str]]): Specific columns to fill. If None, fill all numeric columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with missing values filled.\n\n    Raises:\n        ValueError: If strategy is not recognized or columns contain non-numeric types.\n    \n    Examples:\n        >>> df = pd.DataFrame({'A': [1, np.nan, 3], 'B': [4, 5, np.nan]})\n        >>> fill_missing_values(df, strategy='mean')\n    \"\"\"\n    if strategy not in ['mean', 'median', 'most_frequent']:\n        raise ValueError(f\"Strategy '{strategy}' is not recognized. Use 'mean', 'median', or 'most_frequent'.\")\n\n    imputer = SimpleImputer(strategy=strategy)\n    \n    if columns is None:\n        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n    \n    if not all(col in df.columns for col in columns):\n        raise ValueError(f\"One or more columns in {columns} are not in the DataFrame.\")\n    \n    df[columns] = imputer.fit_transform(df[columns])\n    return df\n\ndef zscore_normalization(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Normalize specified columns using Z-score normalization.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        columns (List[str]): List of column names to normalize.\n\n    Returns:\n        pd.DataFrame: DataFrame with Z-score normalized columns.\n\n    Raises:\n        ValueError: If columns are not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        >>> zscore_normalization(df, ['A'])\n    \"\"\"\n    if not all(col in df.columns for col in columns):\n        raise ValueError(f\"One or more columns in {columns} are not in the DataFrame.\")\n    \n    df[columns] = df[columns].apply(zscore)\n    return df\n\ndef target_encoding(df: pd.DataFrame, target_col: str, categorical_cols: List[str], alpha: float = 0.1) -> pd.DataFrame:\n    \"\"\"\n    Apply target encoding to categorical columns based on the target variable.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        target_col (str): The target column name.\n        categorical_cols (List[str]): List of categorical column names to encode.\n        alpha (float): Smoothing factor to prevent overfitting.\n\n    Returns:\n        pd.DataFrame: DataFrame with target encoded columns.\n\n    Raises:\n        ValueError: If target_col or categorical_cols are not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'Color': ['Red', 'Green', 'Red'], 'Target': [1, 0, 1]})\n        >>> target_encoding(df, 'Target', ['Color'])\n    \"\"\"\n    if target_col not in df.columns or not all(col in df.columns for col in categorical_cols):\n        raise ValueError(f\"Target column '{target_col}' or one of the categorical columns is not in the DataFrame.\")\n    \n    mean_target = df[target_col].mean()\n    \n    encoding_map = df.groupby(categorical_cols)[target_col].mean().to_dict()\n    \n    for col in categorical_cols:\n        df[f'{col}_encoded'] = df[col].map(lambda x: (encoding_map.get(x, mean_target) * (1 - alpha)) + \n                                             (mean_target * alpha))\n    \n    return df\n\ndef interaction_features(df: pd.DataFrame, feature_pairs: List[tuple]) -> pd.DataFrame:\n    \"\"\"\n    Create interaction features from specified pairs of feature columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        feature_pairs (List[tuple]): List of tuples containing pairs of column names.\n\n    Returns:\n        pd.DataFrame: DataFrame with interaction features added.\n\n    Raises:\n        ValueError: If any feature pair contains columns not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n        >>> interaction_features(df, [('A', 'B')])\n    \"\"\"\n    for col1, col2 in feature_pairs:\n        if col1 not in df.columns or col2 not in df.columns:\n            raise ValueError(f\"Feature pair ({col1}, {col2}) contains columns not in the DataFrame.\")\n        \n        df[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n    \n    return df\n```"
    },
    {
        "role": "user",
        "content": "\n## TASK: CONVERT PYTHON TOOLS TO MARKDOWN DOCS ##\n\nYou will receive a complete Python module containing multiple tool functions.\nFor EACH top-level function, produce a markdown section that follows the EXACT format of PROMPT_TOOLSMITH_MARKDOWN_FORMAT.\n\nRules:\n- Use the function name as the tool name\n- Derive parameter info and types from the function signature and docstring\n- Keep the formatting EXACT (labels, backticks, enums, separators) per the spec\n- Output ONLY one concatenated fenced block: ```markdown\\n...sections...\\n```\n- Do not include any prose before or after the fenced block.\n\nREFERENCE FORMAT (do not reproduce here, just follow it):\n\n## REQUIRED MARKDOWN FORMAT ##\nFor each tool, produce EXACT markdown following this structure:\n\n## <tool_name>\n\n**Name:** <tool_name>  \n**Description:** <one-sentence or short paragraph description>  \n**Applicable Situations:** <short phrase describing when to use this tool>\n\n**Parameters:**\n- `<param_name>`:\n  - **Type:** `<type>` (e.g., `pd.DataFrame`, ``string` | `array``, `number`, `boolean`)\n  - **Description:** <clear parameter description>\n  - **Enum:** `value1` | `value2` | `value3`  (ONLY include this line if enum exists)\n  - **Default:** `<value>` or `None`  (ONLY include this line if default exists)\n\n(Repeat the above parameter block for each parameter)\n\n**Required:** `param1`, `param2`  (or write `None` if no required parameters)\n**Result:** <one-sentence description of the successful outcome>  \n**Notes:**\n- <explanatory note 1>\n- <explanatory note 2>\n- <explanatory note 3>\n(Add as many notes as needed to explain assumptions, limitations, edge cases, or best practices)\n\n---\n\nIMPORTANT FORMATTING RULES:\n1. Use exactly two spaces after each colon in bold labels (e.g., \"**Name:**  \")\n2. Parameter type uses backticks with pipes for unions: ``string` | `array``\n3. Enum values separated by ` | ` (space-pipe-space) with backticks around each value\n4. Default values use backticks: `None`, `0.5`, `'auto'`\n5. End each tool section with \"---\" separator (three dashes)\n6. Required parameters listed as comma-separated with backticks: `param1`, `param2`\n7. Notes are bullet points, each starting with \"- \"\n\nExample of correct formatting:\n\n## fill_missing_values\n\n**Name:** fill_missing_values  \n**Description:** Fill missing values in specified columns of a DataFrame. This tool can handle both numerical and categorical features by using different filling methods.  \n**Applicable Situations:** handle missing values in various types of features\n\n**Parameters:**\n- `data`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** A pandas DataFrame object representing the dataset.\n- `columns`:\n  - **Type:** ``string` | `array``\n  - **Description:** The name(s) of the column(s) where missing values should be filled.\n- `method`:\n  - **Type:** `string`\n  - **Description:** The method to use for filling missing values.\n  - **Enum:** `auto` | `mean` | `median` | `mode` | `constant`\n  - **Default:** `auto`\n- `fill_value`:\n  - **Type:** ``number` | `string` | `null``\n  - **Description:** The value to use when method is 'constant'.\n  - **Default:** `None`\n\n**Required:** `data`, `columns`  \n**Result:** Successfully fill missing values in the specified column(s) of data  \n**Notes:**\n- The 'auto' method uses mean for numeric columns and mode for non-numeric columns.\n- Using 'mean' or 'median' on non-numeric columns will raise an error.\n- The 'mode' method uses the most frequent value, which may not always be appropriate.\n- Filling missing values can introduce bias, especially if the data is not missing completely at random.\n- Consider the impact of filling missing values on your analysis and model performance.\n\n---\n\n\n\nHere is the MARKDOWN format you should follow (example):\n```markdown\n## fill_missing_values\n\n**Name:** fill_missing_values  \n**Description:** Fill missing values in specified columns of a DataFrame. This tool can handle both numerical and categorical features by using different filling methods.  \n**Applicable Situations:** handle missing values in various types of features\n\n**Parameters:**\n- `data`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** A pandas DataFrame object representing the dataset.\n- `columns`:\n  - **Type:** ``string` | `array``\n  - **Description:** The name(s) of the column(s) where missing values should be filled.\n- `method`:\n  - **Type:** `string`\n  - **Description:** The method to use for filling missing values.\n  - **Enum:** `auto` | `mean` | `median` | `mode` | `constant`\n  - **Default:** `auto`\n- `fill_value`:\n  - **Type:** ``number` | `string` | `null``\n  - **Description:** The value to use when method is 'constant'.\n  - **Default:** `None`\n\n**Required:** `data`, `columns`  \n**Result:** Successfully fill missing values in the specified column(s) of data  \n**Notes:**\n- The 'auto' method uses mean for numeric columns and mode for non-numeric columns.\n- Using 'mean' or 'median' on non-numeric columns will raise an error.\n- The 'mode' method uses the most frequent value, which may not always be appropriate.\n- Filling missing values can introduce bias, especially if the data is not missing completely at random.\n- Consider the impact of filling missing values on your analysis and model performance.\n\n---\n```\n\nPYTHON MODULE SOURCE:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom scipy.stats import zscore\nfrom typing import List, Union, Dict, Any, Optional\n\ndef create_polynomial_features(df: pd.DataFrame, feature_cols: List[str], degree: int = 2) -> pd.DataFrame:\n    \"\"\"\n    Generate polynomial features from the specified feature columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        feature_cols (List[str]): List of column names to generate polynomial features from.\n        degree (int): The degree of the polynomial features.\n\n    Returns:\n        pd.DataFrame: DataFrame with polynomial features added.\n\n    Raises:\n        ValueError: If feature_cols are not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n        >>> create_polynomial_features(df, ['A', 'B'], degree=2)\n    \"\"\"\n    if not all(col in df.columns for col in feature_cols):\n        raise ValueError(f\"One or more columns in {feature_cols} are not in the DataFrame.\")\n    \n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    poly_features = poly.fit_transform(df[feature_cols])\n    poly_feature_names = poly.get_feature_names_out(feature_cols)\n    \n    return pd.DataFrame(poly_features, columns=poly_feature_names, index=df.index)\n\ndef one_hot_encode(df: pd.DataFrame, categorical_cols: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Perform one-hot encoding on specified categorical columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        categorical_cols (List[str]): List of column names to one-hot encode.\n\n    Returns:\n        pd.DataFrame: DataFrame with one-hot encoded columns added.\n\n    Raises:\n        ValueError: If categorical_cols are not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'Color': ['Red', 'Green'], 'Size': ['S', 'M']})\n        >>> one_hot_encode(df, ['Color', 'Size'])\n    \"\"\"\n    if not all(col in df.columns for col in categorical_cols):\n        raise ValueError(f\"One or more columns in {categorical_cols} are not in the DataFrame.\")\n    \n    encoder = OneHotEncoder(sparse=False, drop='first')\n    encoded_features = encoder.fit_transform(df[categorical_cols])\n    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols), index=df.index)\n    \n    return pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n\ndef fill_missing_values(df: pd.DataFrame, strategy: str = 'mean', columns: Optional[List[str]] = None) -> pd.DataFrame:\n    \"\"\"\n    Fill missing values in numerical columns using specified strategy.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        strategy (str): Strategy to use for filling missing values ('mean', 'median', or 'most_frequent').\n        columns (Optional[List[str]]): Specific columns to fill. If None, fill all numeric columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with missing values filled.\n\n    Raises:\n        ValueError: If strategy is not recognized or columns contain non-numeric types.\n    \n    Examples:\n        >>> df = pd.DataFrame({'A': [1, np.nan, 3], 'B': [4, 5, np.nan]})\n        >>> fill_missing_values(df, strategy='mean')\n    \"\"\"\n    if strategy not in ['mean', 'median', 'most_frequent']:\n        raise ValueError(f\"Strategy '{strategy}' is not recognized. Use 'mean', 'median', or 'most_frequent'.\")\n\n    imputer = SimpleImputer(strategy=strategy)\n    \n    if columns is None:\n        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n    \n    if not all(col in df.columns for col in columns):\n        raise ValueError(f\"One or more columns in {columns} are not in the DataFrame.\")\n    \n    df[columns] = imputer.fit_transform(df[columns])\n    return df\n\ndef zscore_normalization(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Normalize specified columns using Z-score normalization.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        columns (List[str]): List of column names to normalize.\n\n    Returns:\n        pd.DataFrame: DataFrame with Z-score normalized columns.\n\n    Raises:\n        ValueError: If columns are not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        >>> zscore_normalization(df, ['A'])\n    \"\"\"\n    if not all(col in df.columns for col in columns):\n        raise ValueError(f\"One or more columns in {columns} are not in the DataFrame.\")\n    \n    df[columns] = df[columns].apply(zscore)\n    return df\n\ndef target_encoding(df: pd.DataFrame, target_col: str, categorical_cols: List[str], alpha: float = 0.1) -> pd.DataFrame:\n    \"\"\"\n    Apply target encoding to categorical columns based on the target variable.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        target_col (str): The target column name.\n        categorical_cols (List[str]): List of categorical column names to encode.\n        alpha (float): Smoothing factor to prevent overfitting.\n\n    Returns:\n        pd.DataFrame: DataFrame with target encoded columns.\n\n    Raises:\n        ValueError: If target_col or categorical_cols are not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'Color': ['Red', 'Green', 'Red'], 'Target': [1, 0, 1]})\n        >>> target_encoding(df, 'Target', ['Color'])\n    \"\"\"\n    if target_col not in df.columns or not all(col in df.columns for col in categorical_cols):\n        raise ValueError(f\"Target column '{target_col}' or one of the categorical columns is not in the DataFrame.\")\n    \n    mean_target = df[target_col].mean()\n    \n    encoding_map = df.groupby(categorical_cols)[target_col].mean().to_dict()\n    \n    for col in categorical_cols:\n        df[f'{col}_encoded'] = df[col].map(lambda x: (encoding_map.get(x, mean_target) * (1 - alpha)) + \n                                             (mean_target * alpha))\n    \n    return df\n\ndef interaction_features(df: pd.DataFrame, feature_pairs: List[tuple]) -> pd.DataFrame:\n    \"\"\"\n    Create interaction features from specified pairs of feature columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        feature_pairs (List[tuple]): List of tuples containing pairs of column names.\n\n    Returns:\n        pd.DataFrame: DataFrame with interaction features added.\n\n    Raises:\n        ValueError: If any feature pair contains columns not found in the DataFrame.\n    \n    Examples:\n        >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n        >>> interaction_features(df, [('A', 'B')])\n    \"\"\"\n    for col1, col2 in feature_pairs:\n        if col1 not in df.columns or col2 not in df.columns:\n            raise ValueError(f\"Feature pair ({col1}, {col2}) contains columns not in the DataFrame.\")\n        \n        df[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n    \n    return df\n"
    },
    {
        "role": "assistant",
        "content": "```markdown\n## create_polynomial_features\n\n**Name:** create_polynomial_features  \n**Description:** Generate polynomial features from the specified feature columns.  \n**Applicable Situations:** when you need to enhance model performance with polynomial feature interactions\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame containing the data.\n- `feature_cols`:\n  - **Type:** ``string` | `array``\n  - **Description:** List of column names to generate polynomial features from.\n- `degree`:\n  - **Type:** `int`\n  - **Description:** The degree of the polynomial features.\n  - **Default:** `2`\n\n**Required:** `df`, `feature_cols`  \n**Result:** DataFrame with polynomial features added  \n**Notes:**\n- Feature columns must exist in the DataFrame; otherwise, a ValueError is raised.\n- The polynomial features are created without including the bias (constant term).\n- The output DataFrame retains the original DataFrame's index.\n\n---\n\n## one_hot_encode\n\n**Name:** one_hot_encode  \n**Description:** Perform one-hot encoding on specified categorical columns.  \n**Applicable Situations:** when you need to convert categorical variables into a numerical format suitable for machine learning\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame containing the data.\n- `categorical_cols`:\n  - **Type:** ``string` | `array``\n  - **Description:** List of column names to one-hot encode.\n\n**Required:** `df`, `categorical_cols`  \n**Result:** DataFrame with one-hot encoded columns added  \n**Notes:**\n- Categorical columns must exist in the DataFrame; otherwise, a ValueError is raised.\n- The first category for each column is dropped to avoid multicollinearity.\n- The output DataFrame contains both the original and the newly created one-hot encoded columns.\n\n---\n\n## fill_missing_values\n\n**Name:** fill_missing_values  \n**Description:** Fill missing values in numerical columns using specified strategy.  \n**Applicable Situations:** when you need to address missing data issues in your dataset\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame containing the data.\n- `strategy`:\n  - **Type:** `string`\n  - **Description:** Strategy to use for filling missing values.\n  - **Enum:** `mean` | `median` | `most_frequent`\n  - **Default:** `mean`\n- `columns`:\n  - **Type:** ``string` | `array``\n  - **Description:** Specific columns to fill. If `None`, fill all numeric columns.\n  - **Default:** `None`\n\n**Required:** `df`  \n**Result:** DataFrame with missing values filled  \n**Notes:**\n- If the specified strategy is not recognized, a ValueError is raised.\n- If no columns are specified, all numeric columns are filled using the chosen strategy.\n- Using inappropriate strategies for categorical columns will raise an error.\n\n---\n\n## zscore_normalization\n\n**Name:** zscore_normalization  \n**Description:** Normalize specified columns using Z-score normalization.  \n**Applicable Situations:** when you need to standardize features for better model training\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame containing the data.\n- `columns`:\n  - **Type:** ``string` | `array``\n  - **Description:** List of column names to normalize.\n\n**Required:** `df`, `columns`  \n**Result:** DataFrame with Z-score normalized columns  \n**Notes:**\n- Specified columns must exist in the DataFrame; otherwise, a ValueError is raised.\n- The Z-score normalization scales features based on their mean and standard deviation.\n- Normalizing non-numeric columns will raise an error.\n\n---\n\n## target_encoding\n\n**Name:** target_encoding  \n**Description:** Apply target encoding to categorical columns based on the target variable.  \n**Applicable Situations:** when you need to encode categorical variables while considering their relationship with the target variable\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame containing the data.\n- `target_col`:\n  - **Type:** `string`\n  - **Description:** The target column name.\n- `categorical_cols`:\n  - **Type:** ``string` | `array``\n  - **Description:** List of categorical column names to encode.\n- `alpha`:\n  - **Type:** `float`\n  - **Description:** Smoothing factor to prevent overfitting.\n  - **Default:** `0.1`\n\n**Required:** `df`, `target_col`, `categorical_cols`  \n**Result:** DataFrame with target encoded columns  \n**Notes:**\n- Target column and categorical columns must exist in the DataFrame; otherwise, a ValueError is raised.\n- The encoding helps to mitigate overfitting through smoothing.\n- Consider the potential leakage of information from the target variable when using this technique.\n\n---\n\n## interaction_features\n\n**Name:** interaction_features  \n**Description:** Create interaction features from specified pairs of feature columns.  \n**Applicable Situations:** when you want to explore interactions between features to enhance model complexity\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame containing the data.\n- `feature_pairs`:\n  - **Type:** ``array` | `tuple``\n  - **Description:** List of tuples containing pairs of column names.\n\n**Required:** `df`, `feature_pairs`  \n**Result:** DataFrame with interaction features added  \n**Notes:**\n- Each feature pair must exist in the DataFrame; otherwise, a ValueError is raised.\n- Interaction features are created by multiplying the specified pairs of columns.\n- The resulting features can help capture non-linear relationships in the data.\n\n---\n```"
    }
]