Thank you for providing the details, feature list, and tool documentation.  
Below is the **complete, efficient, and well-documented code and rationale for each task in your Data Cleaning phase**, following your plan and the constraints.

---

## TASK 1: Data Type Enforcement & Value Cleaning

**THOUGHT PROCESS:**  
First, we will work on copies of both train and test dataframes. We'll:
- Ensure all numerical columns are float and fill invalids with the (train) median.
- Standardize the Gender column to "Male"/"Female", filling with the train mode if missing/invalid.
- Clean all binary categorical features ("yes"/"no"), filling invalid/missing with the train mode.
- Clean ordinal categorical features (`CAEC`, `CALC`) enforcing the specified order, filling invalid/missing with the mode or "no" as fallback.
- For `MTRANS`, we'll harmonize categories and clean up strings between train and test.

**CODE:**
```python
import os
import pandas as pd

# File paths
data_dir = '/Users/scottiejj/Desktop/AutoKaggle_APAPTED/multi_agents/competition/obesity_risks/'
train_path = os.path.join(data_dir, 'train.csv')
test_path = os.path.join(data_dir, 'test.csv')

# Load data
train = pd.read_csv(train_path)
test = pd.read_csv(test_path)

# Work on copies
train_clean = train.copy()
test_clean = test.copy()

# 1. Numerical columns
num_cols = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']

# Fill invalids with train median (compute median for each col in train, apply to both)
num_medians = {col: train_clean[col].median() for col in num_cols}

for col in num_cols:
    train_clean = numerical_type_enforcer(train_clean, cols=[col], coerce=True, fill_invalid_with=num_medians[col])
    test_clean = numerical_type_enforcer(test_clean, cols=[col], coerce=True, fill_invalid_with=num_medians[col])

# 2. Gender
train_gender_mode = train_clean['Gender'].mode()[0] if train_clean['Gender'].mode().size > 0 else "Female"
train_clean = gender_cleaner(train_clean, col='Gender', valid_values=("Male", "Female"), fill_value=train_gender_mode)
test_clean = gender_cleaner(test_clean, col='Gender', valid_values=("Male", "Female"), fill_value=train_gender_mode)

# 3. Binary Categorical Features
binary_cols = ['family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC']
for col in binary_cols:
    mode = train_clean[col].mode()[0] if train_clean[col].mode().size > 0 else "no"
    train_clean = categorical_binary_cleaner(train_clean, col=col, valid_values=("yes", "no"), fill_value=mode)
    test_clean = categorical_binary_cleaner(test_clean, col=col, valid_values=("yes", "no"), fill_value=mode)

# 4. Ordinal Categorical Features
ordinal_cols = ['CAEC', 'CALC']
ordinal_order = ["no", "Sometimes", "Frequently", "Always"]
for col in ordinal_cols:
    train_mode = train_clean[col].mode()[0] if train_clean[col].mode().size > 0 else "no"
    train_clean = ordinal_categorical_cleaner(train_clean, col=col, valid_order=ordinal_order, fill_value=train_mode)
    test_clean = ordinal_categorical_cleaner(test_clean, col=col, valid_order=ordinal_order, fill_value=train_mode)

# 5. Manual Harmonization: MTRANS
# Normalize string values (strip, lower, replace spaces/underscores for harmonization)
def harmonize_mtrans(series):
    return (
        series.astype(str)
        .str.strip()
        .str.replace(' ', '_')
        .str.replace('/', '_')
        .str.replace('-', '_')
        .str.replace('__', '_')
        .str.lower()
        .str.capitalize()
    )

train_clean['MTRANS'] = harmonize_mtrans(train_clean['MTRANS'])
test_clean['MTRANS'] = harmonize_mtrans(test_clean['MTRANS'])

# Ensure all test categories are present in train, otherwise relabel as 'Other'
valid_mtrans = set(train_clean['MTRANS'].unique())
test_clean['MTRANS'] = test_clean['MTRANS'].apply(lambda x: x if x in valid_mtrans else 'Other')
train_clean['MTRANS'] = train_clean['MTRANS'].apply(lambda x: x if x in valid_mtrans else 'Other')
```

**EXPLANATION:**  
- Loads data and works on copies.
- Numeric columns are coerced to float, invalids filled with train median.
- Gender and binary categoricals are cleaned and harmonized using train mode.
- Ordinal categoricals are enforced for valid order and filled with mode.
- MTRANS is harmonized for spelling/case and rare test-only categories are relabeled as 'Other'.

---

## TASK 2: Impute Missing Values

**THOUGHT PROCESS:**  
After type enforcement, we impute missing values:
- For `Height`, `Weight`, `Age`: impute by median *within* each Gender.
- For other numerical: impute by overall median from train set.
- Categorical missing values should already be filled from Task 1.

**CODE:**
```python
# Numerical imputation by group for relevant columns
group_impute_cols = ['Height', 'Weight', 'Age']
for col in group_impute_cols:
    train_clean = impute_missing_by_group_median(train_clean, target_col=col, group_col='Gender')
    test_clean = impute_missing_by_group_median(test_clean, target_col=col, group_col='Gender')

# Remaining numerical columns: fill any leftover missing with overall train median (already handled with numerical_type_enforcer in practice)
for col in ['FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']:
    # If any missing remains (shouldn't, but check just in case)
    if train_clean[col].isnull().any():
        train_clean[col] = train_clean[col].fillna(num_medians[col])
    if test_clean[col].isnull().any():
        test_clean[col] = test_clean[col].fillna(num_medians[col])
```

**EXPLANATION:**  
- Uses group-median imputation for anthropometric features, ensuring sex-specific distributions are respected.
- Verifies all other numerical columns have no missing values (fills with median if needed, though this should be redundant after Task 1).

---

## TASK 3: Outlier/Anomaly Treatment (BMI and Numerical Features)

**THOUGHT PROCESS:**  
- Calculate BMI and flag rows with implausible BMI.
- Outlier treatment: cap at 1st/99th percentile for mild outliers, drop extreme BMI rows in train, cap in test.
- For all numerical columns, flag IQR outliers and cap them.

**CODE:**
```python
# 1. Add BMI and BMI_flag
train_clean = bmi_validator_and_flagger(train_clean, height_col='Height', weight_col='Weight', bmi_col='BMI', flag_col='BMI_flag')
test_clean = bmi_validator_and_flagger(test_clean, height_col='Height', weight_col='Weight', bmi_col='BMI', flag_col='BMI_flag')

# 2. Handle implausible BMI
# If BMI < 10 or BMI > 80: in train, drop if far outside; in test, cap at 1st/99th percentile
# We'll drop train rows with BMI_flag==True and BMI < 8 or BMI > 90 (very extreme), otherwise cap

far_low, far_high = 8, 90
# For train
extreme_bmi = train_clean[(train_clean['BMI_flag']) & ((train_clean['BMI'] < far_low) | (train_clean['BMI'] > far_high))].index
train_clean = train_clean.drop(extreme_bmi)
# For remaining flagged in train and all flagged in test: cap BMI, Height, Weight at 1st/99th percentile

for col in ['BMI', 'Height', 'Weight']:
    # Compute percentiles from train (excluding dropped rows)
    lower = train_clean[col].quantile(0.01)
    upper = train_clean[col].quantile(0.99)
    train_clean = cap_outliers(train_clean, col=col, lower_quantile=0.01, upper_quantile=0.99)
    test_clean = cap_outliers(test_clean, col=col, lower_quantile=0.01, upper_quantile=0.99)

# 3. IQR outlier flagging and capping for all numerical features (incl. BMI)
iqr_num_cols = num_cols + ['BMI']
for col in iqr_num_cols:
    train_clean = iqr_outlier_flagger(train_clean, col=col, iqr_mult=1.5, flag_col=f'{col}_outlier_flag')
    test_clean = iqr_outlier_flagger(test_clean, col=col, iqr_mult=1.5, flag_col=f'{col}_outlier_flag')
    train_clean = cap_outliers(train_clean, col=col, lower_quantile=0.01, upper_quantile=0.99)
    test_clean = cap_outliers(test_clean, col=col, lower_quantile=0.01, upper_quantile=0.99)
```

**EXPLANATION:**  
- Calculated BMI, flagged implausibles, and dropped truly extreme BMI rows in train.
- For milder outliers, capped values.
- Applied IQR-based outlier flagging and capping for all numerical columns.

---

## TASK 4: Harmonize Categorical Levels & Rare Category Handling

**THOUGHT PROCESS:**  
- For categorical features with >5 categories or rare entries, group rare categories as "Rare".
- Ensure all test categories exist in train; otherwise, relabel as "Rare".
- Harmonize spelling/capitalization.

**CODE:**
```python
# Define categorical columns for rare handling
cat_cols = ['MTRANS', 'CAEC', 'CALC']

for col in cat_cols:
    train_clean = handle_rare_categories(train_clean, col=col, threshold=10, new_category='Rare')
    test_clean = handle_rare_categories(test_clean, col=col, threshold=10, new_category='Rare')
    # Harmonize test categories: if not in train, set to 'Rare'
    valid_cats = set(train_clean[col].unique())
    test_clean[col] = test_clean[col].apply(lambda x: x if x in valid_cats else 'Rare')
    train_clean[col] = train_clean[col].apply(lambda x: x if x in valid_cats else 'Rare')

# Ensure all categorical columns are consistent in spelling/capitalization
# For all categorical columns: strip, capitalize (except binary, which should be 'yes'/'no')
for col in ['MTRANS', 'CAEC', 'CALC']:
    train_clean[col] = train_clean[col].astype(str).str.strip().str.capitalize()
    test_clean[col] = test_clean[col].astype(str).str.strip().str.capitalize()

# For binary: ensure lower case and strip
for col in binary_cols:
    train_clean[col] = train_clean[col].str.strip().str.lower()
    test_clean[col] = test_clean[col].str.strip().str.lower()
```

**EXPLANATION:**  
- Used `handle_rare_categories` to consolidate rare classes.
- Ensured test set never has an unseen label (maps to "Rare").
- Standardized category spelling/capitalization for all categorical features.

---

## SAVE CLEANED DATASETS

**CODE:**
```python
# Save final cleaned datasets
train_clean.to_csv(os.path.join(data_dir, 'cleaned_train.csv'), index=False)
test_clean.to_csv(os.path.join(data_dir, 'cleaned_test.csv'), index=False)
```

---

# SUMMARY

- All data types are enforced and values cleaned.
- Missing values are imputed correctly and consistently.
- Outliers and implausible BMI are handled, with extreme rows dropped in train and capped in test.
- Categorical levels are harmonized, and rare classes are consolidated.
- Cleaned datasets are saved as required.

**Your data is now clean and ready for Feature Engineering.**