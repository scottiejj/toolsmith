[
    {
        "role": "system",
        "content": "You are an excellent toolsmith.\nYou design phase-appropriate tools and their documentation for Planner/Developer to use."
    },
    {
        "role": "user",
        "content": "# INITIAL DATASET BACKGROUND\nYour ultimate goal is to design tools for the dataset described below (initial background):\n# Competition Information\n\n## 1. Competition Overview\nThe \"obesity_risks\" competition is a part of the Kaggle Playground Series, focusing on predicting obesity risk in individuals related to cardiovascular diseases. The competition runs from February 1, 2024, to March 1, 2024, and submissions are evaluated based on accuracy.\n\n## 2. Files\n- **train.csv**: The training dataset containing features and the target variable `NObeyesdad`.\n- **test.csv**: The test dataset for which predictions need to be made.\n- **sample_submission.csv**: A template to demonstrate the required submission format.\n\n## 3. Problem Definition\nParticipants are tasked with predicting the obesity classification of individuals based on various demographic and lifestyle features. The target variable `NObeyesdad` categorizes individuals into different obesity risk levels.\n\n## 4. Data Information\n### 4.1 Data type\n#### 4.1.1 ID type\n- `id`: Unique identifier for each data point (not used in model training).\n\n#### 4.1.2 Numerical type\n- `Age`: Age of individuals (in years).\n- `Height`: Height of individuals (in meters).\n- `Weight`: Weight of individuals (in kilograms).\n- `CH2O`: Daily water intake (liters).\n- `FAF`: Frequency of physical activity.\n- `TUE`: Time spent exercising (in hours).\n- `CALC`: Daily calcium intake (grams).\n\n#### 4.1.3 Categorical type\n- `Gender`: Gender of individuals (Male/Female).\n- `family_history_with_overweight`: Family history of overweight (yes/no).\n- `FAVC`: Food intake frequency (yes/no).\n- `FCVC`: Frequency of vegetable consumption (numerical scale).\n- `NCP`: Number of meals consumed per day (numerical scale).\n- `CAEC`: Consumption of food between meals (categorical).\n- `SMOKE`: Smoking status (yes/no).\n- `SCC`: Self-calculated calories (yes/no).\n- `MTRANS`: Means of transportation (categorical).\n- `NObeyesdad`: Level of obesity (target variable).\n\n#### 4.1.4 Datetime type\n- None.\n\n### 4.2 Detailed data description\nThe dataset consists of training and test segments, capturing demographic and lifestyle information. The target variable `NObeyesdad` classifies individuals into obesity levels ranging from \"Insufficient Weight\" to \"Obesity Type III.\" The training data includes various age groups, height and weight variations, and lifestyle behaviors.\n\n## 5. Target Variable\nThe target variable `NObeyesdad` categorizes obesity levels into the following classes:\n- \"Insufficient Weight\"\n- \"Normal Weight\"\n- \"Overweight Level I\"\n- \"Overweight Level II\"\n- \"Obesity Type I\"\n- \"Obesity Type II\"\n- \"Obesity Type III\"\n\n## 6. Evaluation Metrics\nSubmissions will be evaluated based on the accuracy score, which measures the proportion of correctly predicted classifications.\n\n## 7. Submission Format\nThe submission file must include the following columns:\n- `id`: Unique identifier for each data point.\n- `NObeyesdad`: Predicted obesity classification.\n\nAn example submission format:\n```\nid,NObeyesdad\n20758,Normal_Weight\n20759,Normal_Weight\n20760,Obesity Type I\n```\n\n## 8. Other Key Aspects\n- The competition utilizes a synthetically generated dataset, allowing for a balance between real-world data characteristics and confidentiality of test labels.\n- Participants are encouraged to explore data visualizations, clustering, and exploratory data analysis (EDA) as part of their solutions.\n- Prizes consist of Kaggle merchandise, with limitations on multiple wins by the same individual.\n\n# TASK\n\nYou are now designing ML tools for the current development phase: Feature Engineering.\nYour tools should include BOTH:\n1. Dataset-specific tools: Tailored to the unique characteristics of this dataset\n2. Commonly reusable tools: General-purpose utilities applicable across data science tasks\n\nThe Planner and Developer will use these tools to execute tasks in this phase.\nI will provide you with DATASET BACKGROUND, PHASE Information, and previous reports and plans.\n\nYou can use the following reasoning pattern to design the tools:\n1. Understand the dataset domain and phase objectives.\n2. Identify dataset-specific needs (e.g., domain-specific transformations, dataset-specific patterns)\n3. Identify commonly needed operations for this phase type (e.g., standard EDA plots, generic cleaners)\n4. For each tool, ask yourself and answer:\n    - \"Is this tool dataset-specific or generally reusable?\"\n    - \"What parameters should this tool accept to be flexible?\"\n    - \"Which data types (numerical, categorical, datetime, ID) does this tool handle?\"\n    - \"What are the expected inputs and outputs?\"\n    - \"What constraints or edge cases must the tool handle?\"\n5. Ensure tools are:\n    - Phase-appropriate (EDA tools differ from cleaning or feature engineering tools)\n    - Well-balanced (mix of specific + general utilities)\n    - Well-documented (clear parameter descriptions, applicable situations, and usage notes)\n    - Compliant with README format (Markdown documentation)\n\nDesign as many tools as needed to comprehensively support this phase, but prioritize quality over quantity.\nFocus on tools that will be most useful to the Planner and Developer.\n\nImportant: Strongly prioritize domain-specific (dataset background-aware) tools whenever they provide clear value. Make their domain rationale explicit so users understand why they are beneficial for this dataset.\n\n\n# CONSTRAINTS\n\n## CONSTRAINTS ##\n1. Tool Balance:\n   - Design tools appropriate for phase: Feature Engineering\n   - Include BOTH dataset-specific AND generic data science tools\n   - Prioritize dataset-specific tools when they add measurable value (keep a healthy mix)\n   - Prioritize quality and usefulness over quantity\n   \n2. Technical Requirements:\n   - Use only: pandas, numpy, scikit-learn, scipy, matplotlib, seaborn\n   - No file I/O operations inside tools\n   - No network calls or external dependencies\n   - All tools must be pure functions (no global state modifications)\n\n3. Design Principles:\n   - Parameterize column names (never hard-code feature names)\n   - Provide clear, actionable error messages\n   - Handle edge cases gracefully (empty DataFrames, missing columns, wrong types)\n   - Include type hints for all parameters and return values\n   - Document assumptions and limitations in notes\n   - Mark in description whether tool is \"dataset-specific\" or \"generic\"\n   - For dataset-specific tools, include a brief \"Domain rationale\" note explaining why the tool fits this dataset background\n\n4. Output Requirements:\n   - Each tool MUST have markdown documentation\n   - Follow exact README format for Markdown\n\n\n#############\n# START TOOL DESIGN #\nBefore you begin, please request the following information from me, which contain important information that will guide your tool design for current phase:\n0. CURRENT phase specific tool GUIDELINES\n1. CURRENT phase available features\n2. CURRENT phase data preview\n3. Previous phases' plans and reports\n"
    },
    {
        "role": "assistant",
        "content": "To ensure that the tools I design are optimally tailored for your current phase and dataset, please provide the following information:\n\n1. **CURRENT phase specific tool GUIDELINES**\n    - Any rules, priorities, or best practices you want enforced for Feature Engineering tools in this project.\n\n2. **CURRENT phase available features**\n    - A list of which features/columns are available at this stage (including if any columns have been dropped/engineered already).\n\n3. **CURRENT phase data preview**\n    - A small sample (e.g., 5-10 rows) of your data as it currently exists, including both train and test if possible.\n\n4. **Previous phases' plans and reports**\n    - Summaries or outputs from earlier phases (e.g., EDA findings, data cleaning decisions, issues identified) that might influence what features or transformations are needed.\n\nOnce you provide these, I'll proceed to design a set of tools and their documentation, as requested."
    },
    {
        "role": "user",
        "content": "\n## TASK: PRODUCE A COMPLETE PYTHON MODULE OF TOOLS ##\n\nYou are designing tools for the current phase: Feature Engineering.\n## PHASE-SPECIFIC TOOL GUIDELINES ##:\n\n- Focus on: domain-specific feature creators first, plus transformation utilities, aggregation helpers, and dimensionality reduction tools\n- Example tools: dataset-specific interaction indicators, time-based feature extractor, target encoding with regularization, and a small set of generic transformations\n\n\nUsing the Context provided below, generate a SINGLE Python module that contains multiple functions.\n\nStrict rules:\n- Follow the style of functions in `ml_tools.py` (clear docstrings: Args, Returns, Raises, Examples when helpful)\n- Use only pandas, numpy, scikit-learn, scipy.stats (standard library allowed); no file/network I/O\n- Pure functions: do not mutate global state; return results explicitly\n- Parameterize column names; no hard-coded feature names\n- Validate inputs and raise ValueError/TypeError with actionable messages\n- Type-hint all parameters and return values\n- Consolidate imports at the top; avoid duplicates\n - For the Model Building phase specifically: include at least three `scikit-learn` model wrappers with a common interface (`fit_and_evaluate_<algo>`), and document an explicit standardized Return schema (keys for metrics, `cv_scores`, `model_name`, `estimator`).\n\nOutput format:\n- Return EXACTLY one fenced Python block containing the entire module content\n- Start with imports, then define all functions\n- One function per tool, snake_case names\n- Define and use consistent, unambiguous key names in returned dicts and DataFrame columns; avoid synonyms. Include an explicit \"Return schema\" in the docstring listing exact keys and types.\n\n\nModule composition guidance:\n- Include BOTH dataset-specific and reusable tools for the phase\n- Focus on high-utility, phase-appropriate tools\n - Prefer LESS THAN 10 tools rather than many superficial ones\n- Prefer domain-specific (dataset background-aware) tools where they provide clear benefit; include a brief \"Domain rationale\" in each such function's docstring or notes\n\nContext:\nPhase: Feature Engineering\nCompetition: obesity_risks\nCURRENT phase available features:\n\n#############\n# TARGET VARIABLE #\n['NObeyesdad']\n#############\n# CLEANED TRAIN DATA WITH FEATURES #\nid,Gender,Age,Height,Weight,family_history_with_overweight,FAVC,FCVC,NCP,CAEC,SMOKE,CH2O,SCC,FAF,TUE,CALC,MTRANS,NObeyesdad,BMI,BMI_flag,Age_outlier_flag,Height_outlier_flag,Weight_outlier_flag,FCVC_outlier_flag,NCP_outlier_flag,CH2O_outlier_flag,FAF_outlier_flag,TUE_outlier_flag,BMI_outlier_flag\n\n#############\n# CLEANED TEST DATA WITH FEATURES #\nid,Gender,Age,Height,Weight,family_history_with_overweight,FAVC,FCVC,NCP,CAEC,SMOKE,CH2O,SCC,FAF,TUE,CALC,MTRANS,BMI,BMI_flag,Age_outlier_flag,Height_outlier_flag,Weight_outlier_flag,FCVC_outlier_flag,NCP_outlier_flag,CH2O_outlier_flag,FAF_outlier_flag,TUE_outlier_flag,BMI_outlier_flag\n\nCURRENT phase data preview:\nData preview:\n# Data Information\n\n## Data Type\n\n### ID type\n- `id`  \n  - A unique identifier for each data point.  \n  - **Note:** This column should NOT be used as a feature for model training.\n\n### Numerical type\n- `Age`\n- `Height`\n- `Weight`\n- `FCVC` (Frequency of consumption of vegetables, float)\n- `NCP` (Number of main meals, float)\n- `CH2O` (Daily water intake, float)\n- `FAF` (Physical activity frequency, float)\n- `TUE` (Time using technology devices, float)\n- `BMI` (Body Mass Index, float; derived from Height and Weight)\n- Boolean/flag columns (all False/True, indicating outliers or flags):\n  - `BMI_flag`\n  - `Age_outlier_flag`\n  - `Height_outlier_flag`\n  - `Weight_outlier_flag`\n  - `FCVC_outlier_flag`\n  - `NCP_outlier_flag`\n  - `CH2O_outlier_flag`\n  - `FAF_outlier_flag`\n  - `TUE_outlier_flag`\n  - `BMI_outlier_flag`\n\n### Categorical type\n- `Gender` (Male/Female)\n- `family_history_with_overweight` (yes/no)\n- `FAVC` (Frequent consumption of high caloric food: yes/no)\n- `CAEC` (Consumption of food between meals: Sometimes/Frequently/Always/No)\n- `SMOKE` (yes/no)\n- `SCC` (Monitoring of caloric consumption: yes/no)\n- `CALC` (Consumption of alcohol: Sometimes/No/Always/Frequently)\n- `MTRANS` (Transportation method: Public_transportation/Automobile/etc.)\n- **Train only:** `NObeyesdad` (Obesity level, multiclass label; see Target Variable)\n\n### Datetime type\n- **None** of the provided features are datetime type.\n\n## Detailed data description\n\nThe dataset consists of anthropometric, behavioral, and lifestyle features for individuals, with the goal of predicting obesity levels. The train and test datasets include:\n\n- **Demographics:** `Gender`, `Age`\n- **Physical Measures:** `Height`, `Weight`, `BMI`\n- **Dietary Habits:** `family_history_with_overweight`, `FAVC`, `FCVC`, `NCP`, `CAEC`, `CH2O`, `SCC`, `CALC`\n- **Activity/Behavior:** `FAF`, `TUE`, `MTRANS`, `SMOKE`\n- **Technical Flags:** Several outlier or flag columns (e.g., `BMI_flag`, `Age_outlier_flag`, etc.), which are all boolean and can be used for anomaly detection or feature engineering.\n\nNotable patterns:\n- All features are either numerical or categorical, with many categorical variables encoded as yes/no or levels (e.g., Sometimes, Frequently).\n- BMI is a calculated field and also appears as a separate feature.\n- Outlier flags for most numerical features indicate possible data cleaning steps have been taken.\n- The target (`NObeyesdad`) is multiclass with values such as: `Insufficient_Weight`, `Normal_Weight`, `Overweight_Level_I`, `Overweight_Level_II`, `Obesity_Type_I`, `Obesity_Type_II`, `Obesity_Type_III`.\n- Both train and test data have the same set of features, except the test set does not include the target variable.\n- The dataset appears to be well-cleaned, with outliers flagged rather than removed.\n\n## Target Variable\n- **Variable:** `NObeyesdad`\n- **Type:** Multiclass categorical\n- **Description:** Obesity level classification for each individual. Possible values include:\n  - `Insufficient_Weight`\n  - `Normal_Weight`\n  - `Overweight_Level_I`\n  - `Overweight_Level_II`\n  - `Obesity_Type_I`\n  - `Obesity_Type_II`\n  - `Obesity_Type_III`\n\n## Submission format (if applicable)\n- The submission file should contain:\n  - `id` (integer or string): The unique identifier for each sample (as in test data).\n  - `NObeyesdad` (string): The predicted obesity class for each sample.\n- **Example:**\n\n| id    | NObeyesdad          |\n|-------|---------------------|\n| 20758 | Obesity_Type_I      |\n| 20759 | Overweight_Level_II |\n| ...   | ...                 |\n\n- All columns should be present in the submission, with `NObeyesdad` as the prediction output for each `id` in the test set.\nPrevious Plans and Reports:\n## UNDERSTAND BACKGROUND ##\nThere is no plan in this phase.\n## PRELIMINARY EXPLORATORY DATA ANALYSIS ##\nPreliminary Exploratory Data Analysis Plan\n## PLAN\n\n### STEP 1\nTask: Summarize and audit feature types, basic statistics, unique values, and missingness for all features in both train and test datasets.\nTools, involved features and correct parameters:\n- Tools:\n  - `summarize_numerical_features` for numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`)\n  - `summarize_categorical_features` for categorical features (`Gender`, `family_history_with_overweight`, `FAVC`, `CAEC`, `SMOKE`, `SCC`, `CALC`, `MTRANS`, and `NObeyesdad` for train)\n  - Pandas `.is_unique` and `.isnull().sum()` for `id`\n- Parameters:\n  - Use both train and test dataframes\nExpected output or Impact on data:\n- Text tables summarizing statistics, unique values, and missing counts for each feature group\n- Text summary on `id` column uniqueness and missingness\n- No changes to data; purely descriptive\nConstraints:\n- No data modification\n- Use only text/print outputs for summaries\n\n### STEP 2\nTask: Analyze initial distributions for all features to identify basic patterns and spot potential data entry errors or outliers.\nTools, involved features and correct parameters:\n- Tools:\n  - `plot_numerical_distributions` for numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`)\n    - Parameters: No target hue, `n_cols=3`, `figsize=(16, 8)`\n  - `plot_categorical_counts` for categorical features (`Gender`, `family_history_with_overweight`, `FAVC`, `CAEC`, `SMOKE`, `SCC`, `CALC`, `MTRANS`)\n    - Parameters: `n_cols=3`, `figsize=(16, 8)`\n- Use train data primarily; test data if feasible\nExpected output or Impact on data:\n- Histograms for numerical features and countplots for categorical features (max 10 images in total)\n- Descriptions before each plot\n- No changes to data; visual inspection only\nConstraints:\n- Limit to 10 total images\n- Focus on critical, representative plots\n\n### STEP 3\nTask: Detect and quantify outliers and check consistency of numerical features (including BMI).\nTools, involved features and correct parameters:\n- Tools:\n  - `automated_outlier_detector` for numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`)\n    - Parameters: `method='iqr'`, `threshold=1.5`\n  - `bmi_feature_analysis` for BMI calculation using `Height` and `Weight`\n    - Parameters: `height_col='Height'`, `weight_col='Weight'`\n- Apply to train and test data\nExpected output or Impact on data:\n- Text tables with outlier counts and ratios for each numerical feature\n- BMI summary statistics (mean, std, min, max, quartiles)\n- Text report flagging implausible BMI values\n- No changes to data; purely diagnostic\nConstraints:\n- No correction or removal of data\n- All outputs in text except for BMI summary if plot is critical (counted in image quota)\n\n### STEP 4\nTask: Assess target variable distribution and its relationship with key features in train data.\nTools, involved features and correct parameters:\n- Tools:\n  - `analyze_target_distribution` for `NObeyesdad`\n  - `feature_vs_target_boxplot` for `Age`, `Weight`, `Height`, and BMI vs. `NObeyesdad`\n    - Parameters: one plot per feature, `figsize=(8, 5)`\n  - Pandas crosstab for `Gender` vs. `NObeyesdad`\n- Use train data only\nExpected output or Impact on data:\n- Text table of target class counts and proportions\n- Up to 4 boxplots (key numerical features vs. target, within image quota)\n- Crosstab of `Gender` vs. target (text)\n- Descriptions before each output\n- No data changes; foundational understanding of target and its relationships\nConstraints:\n- Do not exceed 10 total images (across all steps)\n- Prioritize boxplots for BMI and Weight if image quota is tight\n- No deep association or multivariate analysis at this stage\n## DATA CLEANING ##\nData Cleaning Plan\n## PLAN\n\n### STEP 1\nTask: Enforce correct data types and clean invalid values for all features.\nTools, involved features and correct parameters:\n- **numerical_type_enforcer**: For numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`), coerce to float, fill invalids with median (computed on train, applied to both train/test).\n- **gender_cleaner**: For `Gender`, valid values `(\"Male\", \"Female\")`, fill invalid/missing with mode from train.\n- **categorical_binary_cleaner**: For binary categorical features (`family_history_with_overweight`, `FAVC`, `SMOKE`, `SCC`), valid values `(\"yes\", \"no\")`, fill invalid/missing with mode.\n- **ordinal_categorical_cleaner**: For ordinal categorical features (`CAEC`, `CALC`), valid order `[\"no\", \"Sometimes\", \"Frequently\", \"Always\"]`, fill invalid/missing with mode or `\"no\"` if ambiguous.\n- **Manual harmonization**: For `MTRANS`, normalize string values and harmonize categories between train and test.\nExpected output or Impact on data: All features have correct types, no invalid values, and categories are consistent across train and test.\nConstraints: Do not drop or encode features at this stage; changes must be applied identically to train and test (except `NObeyesdad`, which is train-only).\n\n### STEP 2\nTask: Handle and impute missing values for all features.\nTools, involved features and correct parameters:\n- For numerical features, after type enforcement, use **impute_missing_by_group_median**:\n  - For `Height`, `Weight`, `Age`: group by `Gender` for imputation.\n  - For `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`: use overall median.\n- For categorical features, fill missing with mode or specified fill value in related cleaning tools.\nExpected output or Impact on data: No missing values in any feature in either train or test set.\nConstraints: Imputation statistics must be based on train data and applied identically to test; do not use target or future data for imputation.\n\n### STEP 3\nTask: Detect and treat outliers and implausible values in numerical features, especially BMI-related.\nTools, involved features and correct parameters:\n- **bmi_validator_and_flagger**: Calculate BMI from `Height` and `Weight`, flag implausible BMI (`BMI < 10` or `BMI > 80`), add `BMI` and `BMI_flag` columns.\n- For rows with `BMI_flag==True`:\n  - If BMI is just outside thresholds and height/weight plausible, use **cap_outliers** at 1st/99th percentiles.\n  - If BMI is far outside plausible range, drop record (train only) or cap/impute in test.\n- **iqr_outlier_flagger**: For each numerical feature, flag outliers (`iqr_mult=1.5`).\n- **cap_outliers**: Winsorize flagged numerical outliers at 1st/99th percentiles.\nExpected output or Impact on data: No implausible BMI or extreme outliers; distributions are robust and realistic.\nConstraints: Do not drop any rows from test; only cap or impute. Train set may drop records only if BMI is far outside plausible human range.\n\n### STEP 4\nTask: Harmonize categorical feature levels and handle rare categories.\nTools, involved features and correct parameters:\n- **handle_rare_categories**: For categorical features with >5 unique categories or rare entries (especially `MTRANS`, `CAEC`, `CALC`), set `threshold=10`, replace rare classes with `\"Rare\"`.\n- Check that all test categories exist in train; if not, map to `\"Rare\"`.\n- Harmonize spelling/capitalization of all categorical features between train and test.\nExpected output or Impact on data: All categorical features have consistent, clean, and manageable value sets across train and test.\nConstraints: Do not drop rows for rare categories; only relabel. No feature encoding or transformation at this phase.\n## IN-DEPTH EXPLORATORY DATA ANALYSIS ##\nIn-depth Exploratory Data Analysis Plan\n## PLAN\n\n### STEP 1\nTask: Advanced Univariate and Bivariate Analysis by Feature Type  \nTools, involved features and correct parameters:  \n- **Numerical Features** (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`, `BMI`):\n  - `target_stratified_numerical_summary` (for group-wise stats by obesity class)\n  - `target_vs_feature_anova` (test for significant differences across classes)\n- **Categorical Features** (`Gender`, `family_history_with_overweight`, `FAVC`, `CAEC`, `SMOKE`, `SCC`, `CALC`, `MTRANS`):\n  - `summarize_categorical_features` (overview)\n  - `categorical_vs_target_chi2` (association test with target)\nExpected output or Impact on data:  \n- Text-based summary tables showing statistics for each feature stratified by target\n- Textual results of ANOVA and Chi-squared tests, highlighting significant feature-target associations\n- No changes to data\nConstraints:  \n- No data modification\n- Output is text unless patterns are not clear from text alone\n\n### STEP 2\nTask: Obesity Class Segmentation & Lifestyle Subgroup Analysis  \nTools, involved features and correct parameters:  \n- `segment_bmi_by_obesity_class` (BMI stats segmented by obesity class, excluding outliers via `BMI_flag`)\n- `lifestyle_segment_obesity_rate` (obesity prevalence tables for each of: `FAVC`, `SMOKE`, `SCC`, `MTRANS`, `CAEC`)\nExpected output or Impact on data:  \n- Text tables summarizing BMI by obesity class and obesity rates within lifestyle subgroups\n- Highlights of class overlap, labeling issues, and high-risk subgroups\nConstraints:  \n- Results as text tables; only generate a visualization if subgroup differences are dramatic and not clear in text\n\n### STEP 3\nTask: Feature Interaction and Multicollinearity Assessment  \nTools, involved features and correct parameters:  \n- `feature_interaction_heatmap` (all numerical features including `BMI`, excluding outlier flags, method=`pearson`, `figsize=(10, 8)`)\nExpected output or Impact on data:  \n- One heatmap visualizing numerical feature correlations\n- Text interpretation of strongest correlations and implications for redundancy or feature engineering\nConstraints:  \n- Limit to one image (the heatmap); all other outputs in text\n\n### STEP 4\nTask: Key Visualization of Feature-Target Patterns  \nTools, involved features and correct parameters:  \n- For up to three numerical features with strongest ANOVA results, use `feature_vs_target_boxplot` (`figsize=(8, 5)`)\n- If a categorical feature shows a strong chi2 association, optionally use a countplot colored by target\nExpected output or Impact on data:  \n- Up to three boxplots (numerical feature vs. target) and one countplot (categorical feature by target), each with descriptive text\n- Visual confirmation and illustration of non-obvious or strongly discriminative patterns\nConstraints:  \n- Maximum of four images including heatmap from Step 3\n- Each plot must be justified by prior statistical findings\n- All plots accompanied by text descriptions and interpretations\n\nIn-depth Exploratory Data Analysis Report\n# FEATURE INFO\n## TARGET VARIABLE\nNObeyesdad\n## FEATURES BEFORE THIS PHASE\n['id', 'Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS', 'NObeyesdad', 'BMI', 'BMI_flag', 'Age_outlier_flag', 'Height_outlier_flag', 'Weight_outlier_flag', 'FCVC_outlier_flag', 'NCP_outlier_flag', 'CH2O_outlier_flag', 'FAF_outlier_flag', 'TUE_outlier_flag', 'BMI_outlier_flag']\n## FEATURES AFTER THIS PHASE\n['id', 'Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS', 'NObeyesdad', 'BMI', 'BMI_flag', 'Age_outlier_flag', 'Height_outlier_flag', 'Weight_outlier_flag', 'FCVC_outlier_flag', 'NCP_outlier_flag', 'CH2O_outlier_flag', 'FAF_outlier_flag', 'TUE_outlier_flag', 'BMI_outlier_flag']\n# REPORT\n\n## QUESTIONS AND ANSWERS  \n\n### Question 1\nWhat are the key statistical findings regarding each numerical feature in relation to the target variable 'NObeyesdad'?\n### Answer 1\nKey findings from the numerical feature summary stratified by obesity class include:\n- **Age**: Median ages increase with obesity class from `Insufficient_Weight` (19.36) to `Obesity_Type_II` (27.70), with `Obesity_Type_I` and `Overweight_Level_II` reflecting similar median ages (around 24-25).  \n- **Height**: There is less variation in median height across classes, but `Obesity_Type_II` has the lowest median height.\n- **Weight**: The median weight rises significantly from `Insufficient_Weight` (47.78 kg) to `Obesity_Type_III` (102.88 kg), indicating a clear trend of increasing weight with obesity class.\n- **BMI**: The median BMI also shows a significant increase from `Insufficient_Weight` (17.51) to `Obesity_Type_III` (41.75), confirming the primary role of BMI in obesity classification.\n\n### Question 2\nWhich features were involved in this phase? What changes did they undergo? If any feature types were modified, answer which features are modified and how they are modified. If any features were deleted or created, answer which features are deleted or created and provide detailed explanations.\n### Answer 2\nThe features involved in this phase include:\n- **Numerical Features**: `Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`, `BMI` were analyzed for statistical significance and associations with the target variable `NObeyesdad`.\n- **Categorical Features**: `Gender`, `family_history_with_overweight`, `FAVC`, `CAEC`, `SMOKE`, `SCC`, `CALC`, `MTRANS` were summarized and tested for associations with the target variable.\n\nNo features were deleted or modified in this phase. However, exploratory analyses may suggest future feature engineering opportunities based on the insights gained.\n\n### Question 3\nWhat potential feature interactions were identified, and how might they influence the target variable?\n### Answer 3\nThe correlation heatmap revealed several interactions:\n- **Weight and BMI**: These features have a strong positive correlation (0.94), suggesting one could be removed or combined as they convey similar information.\n- **Age and BMI**: A moderate positive correlation (0.34) indicates that as age increases, BMI tends to rise, possibly pointing to lifestyle changes affecting weight over time.\n- **Height and Weight**: A moderate correlation (0.42) indicates taller individuals are likely to weigh more, which could influence BMI and obesity class.\n- **FAF (Physical Activity Frequency) and BMI**: A weak negative correlation (-0.21) suggests that increased physical activity may help lower BMI, thus affecting obesity classifications.\n\n### Question 4\nWhat multicollinearity issues were identified among the numerical features, and which features should be considered for removal or combination?\n### Answer 4\nMulticollinearity issues were observed between:\n- **Weight and BMI**: The high correlation (0.94) suggests one could be removed or combined as they convey similar information.\n- **Height and Weight**: Moderate correlation (0.42) could suggest consideration for removal of one feature depending on the model complexity.\n- **FAF and BMI**: Given the weak correlation, these features do not exhibit significant multicollinearity, but further evaluation in modeling could provide clarity.\n\n### Question 5\nWere there any surprising patterns or insights regarding obesity class segmentation and lifestyle factors that could inform new feature creation?\n### Answer 5\nSurprising insights include:\n- The age distribution showed younger individuals classified as `Insufficient_Weight` and `Normal_Weight`, while obesity classes had a broader age range, suggesting that lifestyle factors may evolve with age.\n- Higher obesity classes corresponded with a lack of calorie monitoring, suggesting potential for a new feature that combines dietary habits and monitoring.\n- The significant outliers in age for `Normal_Weight` and `Obesity_Type_I` indicate that older individuals can exist in lower obesity classes, pointing to the need for age-adjusted lifestyle interventions.\n\n### Question 6\nBased on the visualizations and statistical tests, which features or combinations of features would you prioritize for engineering new variables, and what transformations do you propose?\n### Answer 6\n- **BMI and Weight**: Given their high correlation, creating a composite feature that combines both might reduce redundancy while retaining critical information.\n- **Age and Lifestyle Factors**: Combining age with lifestyle factors (e.g., FAVC and SCC) into a \"Risk Profile\" feature could enhance predictive power regarding obesity risk.\n- **Technology Usage (TUE) and Physical Activity (FAF)**: These could be combined to create a \"Lifestyle Balance\" feature that captures a balance between sedentary and active behavior.\n\nTransformations to consider include:\n- Normalizing or standardizing numerical features to improve model performance.\n- Encoding categorical features using one-hot encoding for better representation in machine learning models.\n\n\n\n"
    },
    {
        "role": "assistant",
        "content": "```python\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Optional, Tuple, Any, Union\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.base import BaseEstimator\nfrom scipy.stats import zscore\n\n####################################\n# Feature Engineering Tools Module #\n# For: obesity_risks Competition   #\n# Phase: Feature Engineering       #\n####################################\n\n# ===============================\n# ===== DATASET-SPECIFIC TOOLS ===\n# ===============================\n\ndef create_bmi_category_feature(\n    df: pd.DataFrame,\n    bmi_col: str = \"BMI\",\n    new_col: str = \"BMI_category\"\n) -> pd.DataFrame:\n    \"\"\"\n    Create a BMI category feature based on WHO BMI thresholds.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        bmi_col (str): Column name for BMI.\n        new_col (str): Name for the new BMI category column.\n\n    Returns:\n        pd.DataFrame: DataFrame with new BMI category column.\n\n    Raises:\n        ValueError: If bmi_col not in df.\n\n    Notes:\n        - Dataset-specific tool.\n        - Domain rationale: Encapsulates medical BMI thresholds, making it easier to combine with other features or model non-linear relationships.\n        - BMI categories:\n            - Underweight: < 18.5\n            - Normal weight: 18.5 - 24.9\n            - Overweight: 25 - 29.9\n            - Obesity I: 30 - 34.9\n            - Obesity II: 35 - 39.9\n            - Obesity III: >= 40\n\n    Example:\n        df = create_bmi_category_feature(df, bmi_col=\"BMI\", new_col=\"BMI_category\")\n    \"\"\"\n    if bmi_col not in df.columns:\n        raise ValueError(f\"Column '{bmi_col}' not found in DataFrame.\")\n    bins = [-np.inf, 18.5, 25, 30, 35, 40, np.inf]\n    labels = [\n        \"Underweight\", \"Normal_weight\", \"Overweight\",\n        \"Obesity_I\", \"Obesity_II\", \"Obesity_III\"\n    ]\n    df = df.copy()\n    df[new_col] = pd.cut(df[bmi_col], bins=bins, labels=labels, right=False)\n    return df\n\ndef create_lifestyle_risk_score(\n    df: pd.DataFrame,\n    favc_col: str = \"FAVC\",\n    caec_col: str = \"CAEC\",\n    calc_col: str = \"CALC\",\n    scc_col: str = \"SCC\",\n    smoke_col: str = \"SMOKE\",\n    fcvc_col: str = \"FCVC\",\n    ch2o_col: str = \"CH2O\",\n    faf_col: str = \"FAF\",\n    tue_col: str = \"TUE\",\n    new_col: str = \"lifestyle_risk_score\"\n) -> pd.DataFrame:\n    \"\"\"\n    Create a composite lifestyle risk score based on dietary and behavioral factors.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        favc_col (str): Column for high-caloric food consumption (yes/no).\n        caec_col (str): Column for food between meals.\n        calc_col (str): Column for alcohol consumption.\n        scc_col (str): Column for caloric monitoring (yes/no).\n        smoke_col (str): Column for smoking (yes/no).\n        fcvc_col (str): Frequency of vegetable consumption (higher is better).\n        ch2o_col (str): Water intake (higher is better).\n        faf_col (str): Physical activity frequency (higher is better).\n        tue_col (str): Time using technology (higher is worse).\n        new_col (str): Name for the new lifestyle risk score column.\n\n    Returns:\n        pd.DataFrame: DataFrame with the new lifestyle risk score column.\n\n    Raises:\n        ValueError: If any required column is missing.\n\n    Notes:\n        - Dataset-specific tool.\n        - Domain rationale: Obesity is influenced by combined lifestyle factors; this score summarizes risk-promoting behaviors for modeling.\n        - The score is a weighted sum; higher values indicate higher risk.\n\n    Return schema:\n        DataFrame with all original columns plus [new_col] (float).\n    \"\"\"\n    required_cols = [favc_col, caec_col, calc_col, scc_col, smoke_col,\n                     fcvc_col, ch2o_col, faf_col, tue_col]\n    for col in required_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n\n    df = df.copy()\n    # Map yes/no to 1/0\n    yes_no_map = {'yes': 1, 'no': 0}\n    favc = df[favc_col].map(yes_no_map)\n    scc = df[scc_col].map(yes_no_map)\n    smoke = df[smoke_col].map(yes_no_map)\n\n    # Map CAEC and CALC to ordinal values\n    caec_map = {'no': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}\n    calc_map = {'No': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}\n    caec = df[caec_col].str.capitalize().map(caec_map)\n    calc = df[calc_col].str.capitalize().map(calc_map)\n\n    # Normalize positive behaviors (higher is better, so negative risk)\n    fcvc = df[fcvc_col].fillna(df[fcvc_col].median())\n    ch2o = df[ch2o_col].fillna(df[ch2o_col].median())\n    faf = df[faf_col].fillna(df[faf_col].median())\n    tue = df[tue_col].fillna(df[tue_col].median())\n\n    # Scale positive behaviors (reverse sign for risk)\n    fcvc_scaled = -zscore(fcvc)\n    ch2o_scaled = -zscore(ch2o)\n    faf_scaled = -zscore(faf)\n    tue_scaled = zscore(tue)\n\n    score = (\n        1.0 * favc.fillna(0) +\n        0.7 * caec.fillna(0) +\n        0.5 * calc.fillna(0) +\n        0.8 * smoke.fillna(0) +\n        0.8 * (1 - scc.fillna(0)) +  # Not monitoring calories increases risk\n        0.5 * fcvc_scaled +\n        0.5 * ch2o_scaled +\n        0.7 * faf_scaled +\n        0.7 * tue_scaled\n    )\n    df[new_col] = score\n    return df\n\ndef create_age_lifestyle_interaction(\n    df: pd.DataFrame,\n    age_col: str = \"Age\",\n    favc_col: str = \"FAVC\",\n    fcvc_col: str = \"FCVC\",\n    faf_col: str = \"FAF\",\n    new_col: str = \"age_lifestyle_interaction\"\n) -> pd.DataFrame:\n    \"\"\"\n    Create an interaction feature between age and composite lifestyle factors.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        age_col (str): Column for age.\n        favc_col (str): High-caloric food consumption (yes/no).\n        fcvc_col (str): Vegetable consumption frequency.\n        faf_col (str): Physical activity frequency.\n        new_col (str): Name for the new interaction column.\n\n    Returns:\n        pd.DataFrame: DataFrame with the new interaction feature.\n\n    Raises:\n        ValueError: If any required column is missing.\n\n    Notes:\n        - Dataset-specific tool.\n        - Domain rationale: Age moderates the impact of lifestyle; this feature captures non-linear risk growth with unhealthy behaviors as age increases.\n\n    Return schema:\n        DataFrame with original columns plus [new_col] (float).\n    \"\"\"\n    required_cols = [age_col, favc_col, fcvc_col, faf_col]\n    for col in required_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n\n    df = df.copy()\n    favc_num = df[favc_col].map({'yes': 1, 'no': 0})\n    fcvc = df[fcvc_col].fillna(df[fcvc_col].median())\n    faf = df[faf_col].fillna(df[faf_col].median())\n    # Higher interaction = older age, more high-cal food, less veggies, less activity\n    interaction = df[age_col] * (favc_num + (fcvc.max() - fcvc) + (faf.max() - faf))\n    df[new_col] = interaction\n    return df\n\ndef create_bmi_weight_interaction(\n    df: pd.DataFrame,\n    bmi_col: str = \"BMI\",\n    weight_col: str = \"Weight\",\n    new_col: str = \"bmi_weight_interaction\"\n) -> pd.DataFrame:\n    \"\"\"\n    Create an interaction feature between BMI and Weight.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        bmi_col (str): BMI column.\n        weight_col (str): Weight column.\n        new_col (str): Name for the new interaction column.\n\n    Returns:\n        pd.DataFrame: DataFrame with new interaction column.\n\n    Raises:\n        ValueError: If any required column is missing.\n\n    Notes:\n        - Dataset-specific tool.\n        - Domain rationale: High collinearity between BMI and Weight; this term may help models capture non-linear relationships or flag redundant information.\n\n    Return schema:\n        DataFrame with original columns plus [new_col] (float).\n    \"\"\"\n    if bmi_col not in df.columns or weight_col not in df.columns:\n        raise ValueError(f\"Columns '{bmi_col}' and/or '{weight_col}' not found in DataFrame.\")\n    df = df.copy()\n    df[new_col] = df[bmi_col] * df[weight_col]\n    return df\n\ndef create_lifestyle_balance_feature(\n    df: pd.DataFrame,\n    faf_col: str = \"FAF\",\n    tue_col: str = \"TUE\",\n    new_col: str = \"lifestyle_balance\"\n) -> pd.DataFrame:\n    \"\"\"\n    Create a feature representing the balance between physical activity and sedentary behavior.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        faf_col (str): Physical activity frequency.\n        tue_col (str): Time using technology devices.\n        new_col (str): Name for new balance column.\n\n    Returns:\n        pd.DataFrame: DataFrame with new lifestyle balance column.\n\n    Raises:\n        ValueError: If required columns are missing.\n\n    Notes:\n        - Dataset-specific tool.\n        - Domain rationale: Obesity risk is affected by the balance of activity (FAF) and sedentary (TUE) time.\n\n    Return schema:\n        DataFrame with original columns plus [new_col] (float).\n    \"\"\"\n    if faf_col not in df.columns or tue_col not in df.columns:\n        raise ValueError(f\"Columns '{faf_col}' and/or '{tue_col}' not found in DataFrame.\")\n    df = df.copy()\n    # Positive values: more activity relative to sedentary, negative: more sedentary\n    df[new_col] = df[faf_col] - df[tue_col]\n    return df\n\n# ===============================\n# == GENERIC (REUSABLE) TOOLS ===\n# ===============================\n\ndef standardize_numerical_features(\n    df: pd.DataFrame,\n    numerical_cols: List[str],\n    suffix: str = \"_std\"\n) -> pd.DataFrame:\n    \"\"\"\n    Standardize numerical features using z-score.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        numerical_cols (List[str]): List of numerical columns to standardize.\n        suffix (str): Suffix to append to standardized columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with additional standardized columns.\n\n    Raises:\n        ValueError: If any column not in df.\n\n    Notes:\n        - Generic tool.\n        - Missing values are ignored in standardization but columns will still be present.\n\n    Return schema:\n        DataFrame with original columns plus [col+suffix] for each col in numerical_cols.\n    \"\"\"\n    for col in numerical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n    df = df.copy()\n    for col in numerical_cols:\n        df[f\"{col}{suffix}\"] = zscore(df[col].astype(float), nan_policy='omit')\n    return df\n\ndef one_hot_encode_columns(\n    df: pd.DataFrame,\n    categorical_cols: List[str],\n    drop_original: bool = False,\n    prefix_sep: str = \"_\"\n) -> pd.DataFrame:\n    \"\"\"\n    One-hot encode specified categorical columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        categorical_cols (List[str]): List of categorical columns to encode.\n        drop_original (bool): If True, drop the original categorical columns.\n        prefix_sep (str): Separator for new column names.\n\n    Returns:\n        pd.DataFrame: DataFrame with one-hot encoded columns.\n\n    Raises:\n        ValueError: If any column not in df.\n\n    Notes:\n        - Generic tool.\n        - All categories (including rare) are encoded.\n\n    Return schema:\n        DataFrame with one-hot encoded columns and optionally original columns dropped.\n    \"\"\"\n    for col in categorical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n\n    df = df.copy()\n    ohe_df = pd.get_dummies(df[categorical_cols], prefix=categorical_cols, prefix_sep=prefix_sep)\n    df = pd.concat([df, ohe_df], axis=1)\n    if drop_original:\n        df = df.drop(columns=categorical_cols)\n    return df\n\ndef pca_feature_reduction(\n    df: pd.DataFrame,\n    feature_cols: List[str],\n    n_components: int = 2,\n    prefix: str = \"PCA_\"\n) -> pd.DataFrame:\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to numerical features.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        feature_cols (List[str]): List of columns to include in PCA.\n        n_components (int): Number of principal components to extract.\n        prefix (str): Prefix for PCA component columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with new PCA component columns.\n\n    Raises:\n        ValueError: If any column not in df, or n_components invalid.\n\n    Notes:\n        - Generic tool.\n        - PCA is fit on input data only.\n\n    Return schema:\n        DataFrame with original columns plus [prefix+'1', ..., prefix+str(n_components)].\n    \"\"\"\n    for col in feature_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n    if n_components < 1 or n_components > len(feature_cols):\n        raise ValueError(\"n_components must be between 1 and number of feature_cols.\")\n\n    df = df.copy()\n    X = df[feature_cols].astype(float).fillna(df[feature_cols].mean())\n    pca = PCA(n_components=n_components, random_state=42)\n    pcs = pca.fit_transform(X)\n    for i in range(n_components):\n        df[f\"{prefix}{i+1}\"] = pcs[:, i]\n    return df\n\ndef aggregate_flag_features(\n    df: pd.DataFrame,\n    flag_cols: List[str],\n    new_col: str = \"n_flags\"\n) -> pd.DataFrame:\n    \"\"\"\n    Aggregate multiple boolean flag columns into a single count feature.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        flag_cols (List[str]): List of boolean flag columns.\n        new_col (str): Name for the aggregated count column.\n\n    Returns:\n        pd.DataFrame: DataFrame with aggregated flag count column.\n\n    Raises:\n        ValueError: If any column not in df.\n\n    Notes:\n        - Generic tool.\n        - Useful for summarizing data quality or outlier flags.\n\n    Return schema:\n        DataFrame with original columns plus [new_col] (int).\n    \"\"\"\n    for col in flag_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n    df = df.copy()\n    df[new_col] = df[flag_cols].sum(axis=1)\n    return df\n\ndef target_mean_encoding(\n    df: pd.DataFrame,\n    categorical_cols: List[str],\n    target_col: str,\n    min_samples_leaf: int = 1,\n    smoothing: float = 1.0,\n    suffix: str = \"_target_enc\"\n) -> pd.DataFrame:\n    \"\"\"\n    Perform target mean encoding for categorical columns (suitable for train only).\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        categorical_cols (List[str]): Categorical columns to encode.\n        target_col (str): Target variable (must be numeric or ordinal encoded).\n        min_samples_leaf (int): Minimum samples to allow for regularization.\n        smoothing (float): Smoothing strength (higher = more global mean).\n        suffix (str): Suffix for new encoded columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with new target-encoded columns.\n\n    Raises:\n        ValueError: If any column not in df, or if target_col is not present.\n\n    Notes:\n        - Generic tool, but must be applied with care (leakage risk).\n        - For multiclass targets, must provide ordinal/numeric encoding.\n        - Should be fit on train, then mapping applied to test.\n\n    Return schema:\n        DataFrame with original columns plus [col+suffix] for each col in categorical_cols (float).\n    \"\"\"\n    for col in categorical_cols + [target_col]:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n    df = df.copy()\n    global_mean = df[target_col].mean()\n    for col in categorical_cols:\n        stats = df.groupby(col)[target_col].agg(['mean', 'count'])\n        smoothing_val = 1 / (1 + np.exp(-(stats['count'] - min_samples_leaf) / smoothing))\n        enc_map = global_mean * (1 - smoothing_val) + stats['mean'] * smoothing_val\n        df[f\"{col}{suffix}\"] = df[col].map(enc_map)\n    return df\n\n# ===============================\n# == END OF MODULE ==\n# ===============================\n```"
    },
    {
        "role": "user",
        "content": "\n## TASK: CONVERT PYTHON TOOLS TO MARKDOWN DOCS ##\n\nYou will receive a complete Python module containing multiple tool functions.\n\nRules:\n- Use the function name as the tool name\n- Derive parameter info and types from the function signature and docstring\n- Keep the formatting EXACT (labels, backticks, enums, separators) per the spec\n- Output ONLY one concatenated fenced block: ```markdown\\n...sections...\\n```\n- Do not include any prose before or after the fenced block.\n- If a function is dataset-specific, include a note line in the Notes section starting with \"- Domain rationale:\" that briefly explains why the tool is suited to this dataset/competition\n\n\nFor EACH top-level function, produce a markdown section that follows the EXACT format of the example below.\n\nHere is the MARKDOWN format you should follow (example):\n```markdown\n## fill_missing_values\n\n**Name:** fill_missing_values  \n**Description:** Fill missing values in specified columns of a DataFrame. This tool can handle both numerical and categorical features by using different filling methods.  \n**Applicable Situations:** handle missing values in various types of features\n\n**Parameters:**\n- `data`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** A pandas DataFrame object representing the dataset.\n- `columns`:\n  - **Type:** ``string` | `array``\n  - **Description:** The name(s) of the column(s) where missing values should be filled.\n- `method`:\n  - **Type:** `string`\n  - **Description:** The method to use for filling missing values.\n  - **Enum:** `auto` | `mean` | `median` | `mode` | `constant`\n  - **Default:** `auto`\n- `fill_value`:\n  - **Type:** ``number` | `string` | `null``\n  - **Description:** The value to use when method is 'constant'.\n  - **Default:** `None`\n\n**Required:** `data`, `columns`  \n**Result:** Successfully fill missing values in the specified column(s) of data  \n**Notes:**\n- The 'auto' method uses mean for numeric columns and mode for non-numeric columns.\n- Using 'mean' or 'median' on non-numeric columns will raise an error.\n- The 'mode' method uses the most frequent value, which may not always be appropriate.\n- Filling missing values can introduce bias, especially if the data is not missing completely at random.\n- Consider the impact of filling missing values on your analysis and model performance.\n\n---\n```\n\nPYTHON MODULE SOURCE:\n{python_source}\n"
    },
    {
        "role": "assistant",
        "content": "```markdown\n## create_bmi_category_feature\n\n**Name:** create_bmi_category_feature  \n**Description:** Create a BMI category feature based on WHO BMI thresholds.  \n**Applicable Situations:** Feature engineering for obesity and health-related datasets; transforming continuous BMI into categorical risk classes.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `bmi_col`:\n  - **Type:** `string`\n  - **Description:** Column name for BMI.\n  - **Default:** `\"BMI\"`\n- `new_col`:\n  - **Type:** `string`\n  - **Description:** Name for the new BMI category column.\n  - **Default:** `\"BMI_category\"`\n\n**Required:** `df`  \n**Result:** DataFrame with new BMI category column  \n**Notes:**\n- Dataset-specific tool.\n- Domain rationale: Encapsulates medical BMI thresholds, making it easier to combine with other features or model non-linear relationships.\n- BMI categories:\n    - Underweight: < 18.5\n    - Normal weight: 18.5 - 24.9\n    - Overweight: 25 - 29.9\n    - Obesity I: 30 - 34.9\n    - Obesity II: 35 - 39.9\n    - Obesity III: >= 40\n\n---\n\n## create_lifestyle_risk_score\n\n**Name:** create_lifestyle_risk_score  \n**Description:** Create a composite lifestyle risk score based on dietary and behavioral factors.  \n**Applicable Situations:** Summing multiple health-related behaviors into a single numerical risk score for predictive modeling.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `favc_col`:\n  - **Type:** `string`\n  - **Description:** Column for high-caloric food consumption (yes/no).\n  - **Default:** `\"FAVC\"`\n- `caec_col`:\n  - **Type:** `string`\n  - **Description:** Column for food between meals.\n  - **Default:** `\"CAEC\"`\n- `calc_col`:\n  - **Type:** `string`\n  - **Description:** Column for alcohol consumption.\n  - **Default:** `\"CALC\"`\n- `scc_col`:\n  - **Type:** `string`\n  - **Description:** Column for caloric monitoring (yes/no).\n  - **Default:** `\"SCC\"`\n- `smoke_col`:\n  - **Type:** `string`\n  - **Description:** Column for smoking (yes/no).\n  - **Default:** `\"SMOKE\"`\n- `fcvc_col`:\n  - **Type:** `string`\n  - **Description:** Frequency of vegetable consumption (higher is better).\n  - **Default:** `\"FCVC\"`\n- `ch2o_col`:\n  - **Type:** `string`\n  - **Description:** Water intake (higher is better).\n  - **Default:** `\"CH2O\"`\n- `faf_col`:\n  - **Type:** `string`\n  - **Description:** Physical activity frequency (higher is better).\n  - **Default:** `\"FAF\"`\n- `tue_col`:\n  - **Type:** `string`\n  - **Description:** Time using technology (higher is worse).\n  - **Default:** `\"TUE\"`\n- `new_col`:\n  - **Type:** `string`\n  - **Description:** Name for the new lifestyle risk score column.\n  - **Default:** `\"lifestyle_risk_score\"`\n\n**Required:** `df`  \n**Result:** DataFrame with the new lifestyle risk score column  \n**Notes:**\n- Dataset-specific tool.\n- Domain rationale: Obesity is influenced by combined lifestyle factors; this score summarizes risk-promoting behaviors for modeling.\n- The score is a weighted sum; higher values indicate higher risk.\n- The score includes both negative (risk) and positive (protective) behaviors, with appropriate scaling.\n\n---\n\n## create_age_lifestyle_interaction\n\n**Name:** create_age_lifestyle_interaction  \n**Description:** Create an interaction feature between age and composite lifestyle factors.  \n**Applicable Situations:** When age is hypothesized to moderate the impact of lifestyle behaviors on obesity risk.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `age_col`:\n  - **Type:** `string`\n  - **Description:** Column for age.\n  - **Default:** `\"Age\"`\n- `favc_col`:\n  - **Type:** `string`\n  - **Description:** High-caloric food consumption (yes/no).\n  - **Default:** `\"FAVC\"`\n- `fcvc_col`:\n  - **Type:** `string`\n  - **Description:** Vegetable consumption frequency.\n  - **Default:** `\"FCVC\"`\n- `faf_col`:\n  - **Type:** `string`\n  - **Description:** Physical activity frequency.\n  - **Default:** `\"FAF\"`\n- `new_col`:\n  - **Type:** `string`\n  - **Description:** Name for the new interaction column.\n  - **Default:** `\"age_lifestyle_interaction\"`\n\n**Required:** `df`  \n**Result:** DataFrame with the new interaction feature  \n**Notes:**\n- Dataset-specific tool.\n- Domain rationale: Age moderates the impact of lifestyle; this feature captures non-linear risk growth with unhealthy behaviors as age increases.\n- Higher values represent older individuals with unhealthy behavior combinations.\n\n---\n\n## create_bmi_weight_interaction\n\n**Name:** create_bmi_weight_interaction  \n**Description:** Create an interaction feature between BMI and Weight.  \n**Applicable Situations:** When high correlation between features suggests an interaction term may improve model flexibility or flag redundancy.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `bmi_col`:\n  - **Type:** `string`\n  - **Description:** BMI column.\n  - **Default:** `\"BMI\"`\n- `weight_col`:\n  - **Type:** `string`\n  - **Description:** Weight column.\n  - **Default:** `\"Weight\"`\n- `new_col`:\n  - **Type:** `string`\n  - **Description:** Name for the new interaction column.\n  - **Default:** `\"bmi_weight_interaction\"`\n\n**Required:** `df`  \n**Result:** DataFrame with new interaction column  \n**Notes:**\n- Dataset-specific tool.\n- Domain rationale: High collinearity between BMI and Weight; this term may help models capture non-linear relationships or flag redundant information.\n\n---\n\n## create_lifestyle_balance_feature\n\n**Name:** create_lifestyle_balance_feature  \n**Description:** Create a feature representing the balance between physical activity and sedentary behavior.  \n**Applicable Situations:** When predicting outcomes influenced by physical activity and sedentary time.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `faf_col`:\n  - **Type:** `string`\n  - **Description:** Physical activity frequency.\n  - **Default:** `\"FAF\"`\n- `tue_col`:\n  - **Type:** `string`\n  - **Description:** Time using technology devices.\n  - **Default:** `\"TUE\"`\n- `new_col`:\n  - **Type:** `string`\n  - **Description:** Name for new balance column.\n  - **Default:** `\"lifestyle_balance\"`\n\n**Required:** `df`  \n**Result:** DataFrame with new lifestyle balance column  \n**Notes:**\n- Dataset-specific tool.\n- Domain rationale: Obesity risk is affected by the balance of activity (FAF) and sedentary (TUE) time.\n- Positive values indicate more physical activity relative to sedentary behavior.\n\n---\n\n## standardize_numerical_features\n\n**Name:** standardize_numerical_features  \n**Description:** Standardize numerical features using z-score.  \n**Applicable Situations:** When features need to be on a comparable scale for modeling, distance-based algorithms, or PCA.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `numerical_cols`:\n  - **Type:** `array`\n  - **Description:** List of numerical columns to standardize.\n- `suffix`:\n  - **Type:** `string`\n  - **Description:** Suffix to append to standardized columns.\n  - **Default:** `\"_std\"`\n\n**Required:** `df`, `numerical_cols`  \n**Result:** DataFrame with original columns plus standardized versions  \n**Notes:**\n- Generic tool.\n- Missing values are ignored in standardization; standardized columns will contain NaN where data was missing.\n\n---\n\n## one_hot_encode_columns\n\n**Name:** one_hot_encode_columns  \n**Description:** One-hot encode specified categorical columns.  \n**Applicable Situations:** Preparing categorical features for use in machine learning models that require numeric input.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `categorical_cols`:\n  - **Type:** `array`\n  - **Description:** List of categorical columns to encode.\n- `drop_original`:\n  - **Type:** `bool`\n  - **Description:** If True, drop the original categorical columns.\n  - **Default:** `False`\n- `prefix_sep`:\n  - **Type:** `string`\n  - **Description:** Separator for new column names.\n  - **Default:** `\"_\"`\n\n**Required:** `df`, `categorical_cols`  \n**Result:** DataFrame with one-hot encoded columns (and optionally originals dropped)  \n**Notes:**\n- Generic tool.\n- All unique categories present in the data are encoded.\n\n---\n\n## pca_feature_reduction\n\n**Name:** pca_feature_reduction  \n**Description:** Apply Principal Component Analysis (PCA) to numerical features.  \n**Applicable Situations:** Dimensionality reduction, visualization, or creating orthogonal features for downstream modeling.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `feature_cols`:\n  - **Type:** `array`\n  - **Description:** List of columns to include in PCA.\n- `n_components`:\n  - **Type:** `int`\n  - **Description:** Number of principal components to extract.\n  - **Default:** `2`\n- `prefix`:\n  - **Type:** `string`\n  - **Description:** Prefix for PCA component columns.\n  - **Default:** `\"PCA_\"`\n\n**Required:** `df`, `feature_cols`  \n**Result:** DataFrame with original columns plus new PCA component columns  \n**Notes:**\n- Generic tool.\n- PCA is fit on the supplied data only, not suitable for train-test splits without fitting on train and applying to test.\n\n---\n\n## aggregate_flag_features\n\n**Name:** aggregate_flag_features  \n**Description:** Aggregate multiple boolean flag columns into a single count feature.  \n**Applicable Situations:** Summarizing the number of flags/outliers/anomaly indicators per row.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `flag_cols`:\n  - **Type:** `array`\n  - **Description:** List of boolean flag columns.\n- `new_col`:\n  - **Type:** `string`\n  - **Description:** Name for the aggregated count column.\n  - **Default:** `\"n_flags\"`\n\n**Required:** `df`, `flag_cols`  \n**Result:** DataFrame with original columns plus aggregated flag count  \n**Notes:**\n- Generic tool.\n- Summing over boolean columns; result is integer count per row.\n\n---\n\n## target_mean_encoding\n\n**Name:** target_mean_encoding  \n**Description:** Perform target mean encoding for categorical columns (suitable for train only).  \n**Applicable Situations:** Encoding high-cardinality categorical variables for regression or classification using the target's mean.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input DataFrame.\n- `categorical_cols`:\n  - **Type:** `array`\n  - **Description:** Categorical columns to encode.\n- `target_col`:\n  - **Type:** `string`\n  - **Description:** Target variable (must be numeric or ordinal encoded).\n- `min_samples_leaf`:\n  - **Type:** `int`\n  - **Description:** Minimum samples to allow for regularization.\n  - **Default:** `1`\n- `smoothing`:\n  - **Type:** `float`\n  - **Description:** Smoothing strength (higher = more global mean).\n  - **Default:** `1.0`\n- `suffix`:\n  - **Type:** `string`\n  - **Description:** Suffix for new encoded columns.\n  - **Default:** `\"_target_enc\"`\n\n**Required:** `df`, `categorical_cols`, `target_col`  \n**Result:** DataFrame with original columns plus target-encoded columns  \n**Notes:**\n- Generic tool, but must be applied with care to avoid target leakage (do not fit on full train+test).\n- For multiclass targets, must provide ordinal/numeric encoding.\n- Fit on train only; apply mapping to test.\n\n---\n```"
    }
]