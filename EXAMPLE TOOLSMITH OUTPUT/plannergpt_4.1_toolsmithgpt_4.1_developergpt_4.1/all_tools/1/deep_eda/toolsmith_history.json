[
    {
        "role": "system",
        "content": "You are an excellent toolsmith.\nYou design phase-appropriate tools and their documentation for Planner/Developer to use."
    },
    {
        "role": "user",
        "content": "# INITIAL DATASET BACKGROUND\nYour ultimate goal is to design tools for the dataset described below (initial background):\n# Competition Information\n\n## 1. Competition Overview\nThe \"obesity_risks\" competition is a part of the Kaggle Playground Series, focusing on predicting obesity risk in individuals related to cardiovascular diseases. The competition runs from February 1, 2024, to March 1, 2024, and submissions are evaluated based on accuracy.\n\n## 2. Files\n- **train.csv**: The training dataset containing features and the target variable `NObeyesdad`.\n- **test.csv**: The test dataset for which predictions need to be made.\n- **sample_submission.csv**: A template to demonstrate the required submission format.\n\n## 3. Problem Definition\nParticipants are tasked with predicting the obesity classification of individuals based on various demographic and lifestyle features. The target variable `NObeyesdad` categorizes individuals into different obesity risk levels.\n\n## 4. Data Information\n### 4.1 Data type\n#### 4.1.1 ID type\n- `id`: Unique identifier for each data point (not used in model training).\n\n#### 4.1.2 Numerical type\n- `Age`: Age of individuals (in years).\n- `Height`: Height of individuals (in meters).\n- `Weight`: Weight of individuals (in kilograms).\n- `CH2O`: Daily water intake (liters).\n- `FAF`: Frequency of physical activity.\n- `TUE`: Time spent exercising (in hours).\n- `CALC`: Daily calcium intake (grams).\n\n#### 4.1.3 Categorical type\n- `Gender`: Gender of individuals (Male/Female).\n- `family_history_with_overweight`: Family history of overweight (yes/no).\n- `FAVC`: Food intake frequency (yes/no).\n- `FCVC`: Frequency of vegetable consumption (numerical scale).\n- `NCP`: Number of meals consumed per day (numerical scale).\n- `CAEC`: Consumption of food between meals (categorical).\n- `SMOKE`: Smoking status (yes/no).\n- `SCC`: Self-calculated calories (yes/no).\n- `MTRANS`: Means of transportation (categorical).\n- `NObeyesdad`: Level of obesity (target variable).\n\n#### 4.1.4 Datetime type\n- None.\n\n### 4.2 Detailed data description\nThe dataset consists of training and test segments, capturing demographic and lifestyle information. The target variable `NObeyesdad` classifies individuals into obesity levels ranging from \"Insufficient Weight\" to \"Obesity Type III.\" The training data includes various age groups, height and weight variations, and lifestyle behaviors.\n\n## 5. Target Variable\nThe target variable `NObeyesdad` categorizes obesity levels into the following classes:\n- \"Insufficient Weight\"\n- \"Normal Weight\"\n- \"Overweight Level I\"\n- \"Overweight Level II\"\n- \"Obesity Type I\"\n- \"Obesity Type II\"\n- \"Obesity Type III\"\n\n## 6. Evaluation Metrics\nSubmissions will be evaluated based on the accuracy score, which measures the proportion of correctly predicted classifications.\n\n## 7. Submission Format\nThe submission file must include the following columns:\n- `id`: Unique identifier for each data point.\n- `NObeyesdad`: Predicted obesity classification.\n\nAn example submission format:\n```\nid,NObeyesdad\n20758,Normal_Weight\n20759,Normal_Weight\n20760,Obesity Type I\n```\n\n## 8. Other Key Aspects\n- The competition utilizes a synthetically generated dataset, allowing for a balance between real-world data characteristics and confidentiality of test labels.\n- Participants are encouraged to explore data visualizations, clustering, and exploratory data analysis (EDA) as part of their solutions.\n- Prizes consist of Kaggle merchandise, with limitations on multiple wins by the same individual.\n\n# TASK\n\nYou are now designing ML tools for the current development phase: In-depth Exploratory Data Analysis.\nYour tools should include BOTH:\n1. Dataset-specific tools: Tailored to the unique characteristics of this dataset\n2. Commonly reusable tools: General-purpose utilities applicable across data science tasks\n\nThe Planner and Developer will use these tools to execute tasks in this phase.\nI will provide you with DATASET BACKGROUND, PHASE Information, and previous reports and plans.\n\nYou can use the following reasoning pattern to design the tools:\n1. Understand the dataset domain and phase objectives.\n2. Identify dataset-specific needs (e.g., domain-specific transformations, dataset-specific patterns)\n3. Identify commonly needed operations for this phase type (e.g., standard EDA plots, generic cleaners)\n4. For each tool, ask yourself and answer:\n    - \"Is this tool dataset-specific or generally reusable?\"\n    - \"What parameters should this tool accept to be flexible?\"\n    - \"Which data types (numerical, categorical, datetime, ID) does this tool handle?\"\n    - \"What are the expected inputs and outputs?\"\n    - \"What constraints or edge cases must the tool handle?\"\n5. Ensure tools are:\n    - Phase-appropriate (EDA tools differ from cleaning or feature engineering tools)\n    - Well-balanced (mix of specific + general utilities)\n    - Well-documented (clear parameter descriptions, applicable situations, and usage notes)\n    - Compliant with README format (Markdown documentation)\n\nDesign as many tools as needed to comprehensively support this phase, but prioritize quality over quantity.\nFocus on tools that will be most useful to the Planner and Developer.\n\nImportant: Strongly prioritize domain-specific (dataset background-aware) tools whenever they provide clear value. Make their domain rationale explicit so users understand why they are beneficial for this dataset.\n\n\n# CONSTRAINTS\n\n## CONSTRAINTS ##\n1. Tool Balance:\n   - Design tools appropriate for phase: In-depth Exploratory Data Analysis\n   - Include BOTH dataset-specific AND generic data science tools\n   - Prioritize dataset-specific tools when they add measurable value (keep a healthy mix)\n   - Prioritize quality and usefulness over quantity\n   \n2. Technical Requirements:\n   - Use only: pandas, numpy, scikit-learn, scipy, matplotlib, seaborn\n   - No file I/O operations inside tools\n   - No network calls or external dependencies\n   - All tools must be pure functions (no global state modifications)\n\n3. Design Principles:\n   - Parameterize column names (never hard-code feature names)\n   - Provide clear, actionable error messages\n   - Handle edge cases gracefully (empty DataFrames, missing columns, wrong types)\n   - Include type hints for all parameters and return values\n   - Document assumptions and limitations in notes\n   - Mark in description whether tool is \"dataset-specific\" or \"generic\"\n   - For dataset-specific tools, include a brief \"Domain rationale\" note explaining why the tool fits this dataset background\n\n4. Output Requirements:\n   - Each tool MUST have markdown documentation\n   - Follow exact README format for Markdown\n\n\n#############\n# START TOOL DESIGN #\nBefore you begin, please request the following information from me, which contain important information that will guide your tool design for current phase:\n0. CURRENT phase specific tool GUIDELINES\n1. CURRENT phase available features\n2. CURRENT phase data preview\n3. Previous phases' plans and reports\n"
    },
    {
        "role": "assistant",
        "content": "Thank you! Before designing the tools, please provide the following information for the current \"In-depth Exploratory Data Analysis\" phase:\n\n0. CURRENT phase specific tool GUIDELINES  \n1. CURRENT phase available features  \n2. CURRENT phase data preview  \n3. Previous phases' plans and reports  \n\nOnce I have this context, I will proceed to design a balanced and high-quality set of EDA tools as requested."
    },
    {
        "role": "user",
        "content": "\n## TASK: PRODUCE A COMPLETE PYTHON MODULE OF TOOLS ##\n\nYou are designing tools for the current phase: In-depth Exploratory Data Analysis.\n## PHASE-SPECIFIC TOOL GUIDELINES ##:\n\n- Focus on: advanced statistical analysis, interaction detection, segmentation helpers, explicitly tailored to dataset domain\n- Example tools: feature interaction heatmap generator, target stratified analysis, statistical test automation, dataset-specific segment analyses\n- Visualization tools MUST NOT display figures (no `plt.show()`). They should return the plot object (figure/axes) for the caller to save.\n\n\nUsing the Context provided below, generate a SINGLE Python module that contains multiple functions.\n\nStrict rules:\n- Follow the style of functions in `ml_tools.py` (clear docstrings: Args, Returns, Raises, Examples when helpful)\n- Use only pandas, numpy, scikit-learn, scipy.stats (standard library allowed); no file/network I/O\n- Pure functions: do not mutate global state; return results explicitly\n- Parameterize column names; no hard-coded feature names\n- Validate inputs and raise ValueError/TypeError with actionable messages\n- Type-hint all parameters and return values\n- Consolidate imports at the top; avoid duplicates\n - For the Model Building phase specifically: include at least three `scikit-learn` model wrappers with a common interface (`fit_and_evaluate_<algo>`), and document an explicit standardized Return schema (keys for metrics, `cv_scores`, `model_name`, `estimator`).\n\nOutput format:\n- Return EXACTLY one fenced Python block containing the entire module content\n- Start with imports, then define all functions\n- One function per tool, snake_case names\n- Define and use consistent, unambiguous key names in returned dicts and DataFrame columns; avoid synonyms. Include an explicit \"Return schema\" in the docstring listing exact keys and types.\n\n\nModule composition guidance:\n- Include BOTH dataset-specific and reusable tools for the phase\n- Focus on high-utility, phase-appropriate tools\n - Prefer LESS THAN 10 tools rather than many superficial ones\n- Prefer domain-specific (dataset background-aware) tools where they provide clear benefit; include a brief \"Domain rationale\" in each such function's docstring or notes\n\nContext:\nPhase: In-depth Exploratory Data Analysis\nCompetition: obesity_risks\nCURRENT phase available features:\n\n#############\n# TARGET VARIABLE #\n['NObeyesdad']\n#############\n# CLEANED TRAIN DATA WITH FEATURES #\nid,Gender,Age,Height,Weight,family_history_with_overweight,FAVC,FCVC,NCP,CAEC,SMOKE,CH2O,SCC,FAF,TUE,CALC,MTRANS,NObeyesdad,BMI,BMI_flag,Age_outlier_flag,Height_outlier_flag,Weight_outlier_flag,FCVC_outlier_flag,NCP_outlier_flag,CH2O_outlier_flag,FAF_outlier_flag,TUE_outlier_flag,BMI_outlier_flag\n\n#############\n# CLEANED TEST DATA WITH FEATURES #\nid,Gender,Age,Height,Weight,family_history_with_overweight,FAVC,FCVC,NCP,CAEC,SMOKE,CH2O,SCC,FAF,TUE,CALC,MTRANS,BMI,BMI_flag,Age_outlier_flag,Height_outlier_flag,Weight_outlier_flag,FCVC_outlier_flag,NCP_outlier_flag,CH2O_outlier_flag,FAF_outlier_flag,TUE_outlier_flag,BMI_outlier_flag\n\nCURRENT phase data preview:\nData preview:\n# Data Information\n\n## Data Type\n\n### ID type\n- `id`: Unique identifier for each data point. This column should NOT be used as a feature for model training.\n\n### Numerical type\n- `Age`: Age of the individual (float).\n- `Height`: Height in meters (float).\n- `Weight`: Weight in kilograms (float).\n- `FCVC`: Frequency of consumption of vegetables (float).\n- `NCP`: Number of main meals (float).\n- `CH2O`: Water consumption (liters, float).\n- `FAF`: Physical activity frequency (float, hours per week or similar).\n- `TUE`: Time using technology devices (float, hours per day or similar).\n- `BMI`: Calculated Body Mass Index (float).\n\n**Flag/Outlier Columns (Boolean, but numerical in storage):**\n- `BMI_flag`\n- `Age_outlier_flag`\n- `Height_outlier_flag`\n- `Weight_outlier_flag`\n- `FCVC_outlier_flag`\n- `NCP_outlier_flag`\n- `CH2O_outlier_flag`\n- `FAF_outlier_flag`\n- `TUE_outlier_flag`\n- `BMI_outlier_flag`\n\n### Categorical type\n- `Gender`: 'Male' or 'Female'.\n- `family_history_with_overweight`: 'yes' or 'no'.\n- `FAVC`: Frequent consumption of high calorie food ('yes' or 'no').\n- `CAEC`: Consumption of food between meals ('No', 'Sometimes', 'Frequently', 'Always').\n- `SMOKE`: 'yes' or 'no'.\n- `SCC`: Monitors calorie consumption ('yes' or 'no').\n- `CALC`: Consumption of alcohol ('No', 'Sometimes', etc.).\n- `MTRANS`: Transportation used ('Automobile', 'Public_transportation', etc.).\n- `NObeyesdad`: Obesity level (target, e.g., 'Normal_Weight', 'Overweight_Level_I', etc.). **(train set only)**\n\n### Datetime type\n- No datetime features are present in the data.\n\n## Detailed data description\n\n- The dataset contains health, lifestyle, and demographic features for individuals, aimed at predicting obesity levels.\n- Each record is uniquely identified by the `id` column.\n- The main features include anthropometric data (`Age`, `Height`, `Weight`, `BMI`), dietary habits (`FAVC`, `FCVC`, `NCP`, `CAEC`), lifestyle factors (`SMOKE`, `CH2O`, `SCC`, `FAF`, `TUE`, `CALC`, `MTRANS`), and family history (`family_history_with_overweight`).\n- The `BMI` is pre-computed for each individual and is used to flag obesity-related outliers via the `BMI_flag` and `BMI_outlier_flag`.\n- Several features have associated outlier flags (e.g., `Age_outlier_flag`, `FCVC_outlier_flag`, etc.), which are boolean indicators of anomalous values.\n- The target variable (`NObeyesdad`) classifies individuals into various obesity categories such as `Insufficient_Weight`, `Normal_Weight`, `Overweight_Level_I`, `Overweight_Level_II`, `Obesity_Type_I`, `Obesity_Type_II`, and `Obesity_Type_III`.\n- The dataset appears to be well-cleaned, with no missing values in the sample provided, and ready for modeling.\n- In the test set, the target variable column is absent, aligning with common machine learning practice.\n\n## Target Variable\n\n- **Name:** `NObeyesdad`\n- **Description:** Categorical variable indicating the individual's obesity level, with possible classes such as:\n  - `Insufficient_Weight`\n  - `Normal_Weight`\n  - `Overweight_Level_I`\n  - `Overweight_Level_II`\n  - `Obesity_Type_I`\n  - `Obesity_Type_II`\n  - `Obesity_Type_III`\n\n## Submission format (if applicable)\n\n- The submission file typically should include:\n  - `id` (integer): Unique identifier for each row (as in the test set).\n  - `NObeyesdad` (string): Predicted obesity level for each individual.\n- Example submission:\n  | id    | NObeyesdad         |\n  |-------|--------------------|\n  | 20758 | Obesity_Type_I     |\n  | 20759 | Normal_Weight      |\n  | ...   | ...                |\n- All columns except `id` and the predicted `NObeyesdad` should be excluded from the submission file.\nPrevious Plans and Reports:\n## UNDERSTAND BACKGROUND ##\nThere is no plan in this phase.\n## PRELIMINARY EXPLORATORY DATA ANALYSIS ##\nPreliminary Exploratory Data Analysis Plan\n## PLAN\n\n### STEP 1\nTask: Summarize and audit feature types, basic statistics, unique values, and missingness for all features in both train and test datasets.\nTools, involved features and correct parameters:\n- Tools:\n  - `summarize_numerical_features` for numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`)\n  - `summarize_categorical_features` for categorical features (`Gender`, `family_history_with_overweight`, `FAVC`, `CAEC`, `SMOKE`, `SCC`, `CALC`, `MTRANS`, and `NObeyesdad` for train)\n  - Pandas `.is_unique` and `.isnull().sum()` for `id`\n- Parameters:\n  - Use both train and test dataframes\nExpected output or Impact on data:\n- Text tables summarizing statistics, unique values, and missing counts for each feature group\n- Text summary on `id` column uniqueness and missingness\n- No changes to data; purely descriptive\nConstraints:\n- No data modification\n- Use only text/print outputs for summaries\n\n### STEP 2\nTask: Analyze initial distributions for all features to identify basic patterns and spot potential data entry errors or outliers.\nTools, involved features and correct parameters:\n- Tools:\n  - `plot_numerical_distributions` for numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`)\n    - Parameters: No target hue, `n_cols=3`, `figsize=(16, 8)`\n  - `plot_categorical_counts` for categorical features (`Gender`, `family_history_with_overweight`, `FAVC`, `CAEC`, `SMOKE`, `SCC`, `CALC`, `MTRANS`)\n    - Parameters: `n_cols=3`, `figsize=(16, 8)`\n- Use train data primarily; test data if feasible\nExpected output or Impact on data:\n- Histograms for numerical features and countplots for categorical features (max 10 images in total)\n- Descriptions before each plot\n- No changes to data; visual inspection only\nConstraints:\n- Limit to 10 total images\n- Focus on critical, representative plots\n\n### STEP 3\nTask: Detect and quantify outliers and check consistency of numerical features (including BMI).\nTools, involved features and correct parameters:\n- Tools:\n  - `automated_outlier_detector` for numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`)\n    - Parameters: `method='iqr'`, `threshold=1.5`\n  - `bmi_feature_analysis` for BMI calculation using `Height` and `Weight`\n    - Parameters: `height_col='Height'`, `weight_col='Weight'`\n- Apply to train and test data\nExpected output or Impact on data:\n- Text tables with outlier counts and ratios for each numerical feature\n- BMI summary statistics (mean, std, min, max, quartiles)\n- Text report flagging implausible BMI values\n- No changes to data; purely diagnostic\nConstraints:\n- No correction or removal of data\n- All outputs in text except for BMI summary if plot is critical (counted in image quota)\n\n### STEP 4\nTask: Assess target variable distribution and its relationship with key features in train data.\nTools, involved features and correct parameters:\n- Tools:\n  - `analyze_target_distribution` for `NObeyesdad`\n  - `feature_vs_target_boxplot` for `Age`, `Weight`, `Height`, and BMI vs. `NObeyesdad`\n    - Parameters: one plot per feature, `figsize=(8, 5)`\n  - Pandas crosstab for `Gender` vs. `NObeyesdad`\n- Use train data only\nExpected output or Impact on data:\n- Text table of target class counts and proportions\n- Up to 4 boxplots (key numerical features vs. target, within image quota)\n- Crosstab of `Gender` vs. target (text)\n- Descriptions before each output\n- No data changes; foundational understanding of target and its relationships\nConstraints:\n- Do not exceed 10 total images (across all steps)\n- Prioritize boxplots for BMI and Weight if image quota is tight\n- No deep association or multivariate analysis at this stage\n## DATA CLEANING ##\nData Cleaning Plan\n## PLAN\n\n### STEP 1\nTask: Enforce correct data types and clean invalid values for all features.\nTools, involved features and correct parameters:\n- **numerical_type_enforcer**: For numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`), coerce to float, fill invalids with median (computed on train, applied to both train/test).\n- **gender_cleaner**: For `Gender`, valid values `(\"Male\", \"Female\")`, fill invalid/missing with mode from train.\n- **categorical_binary_cleaner**: For binary categorical features (`family_history_with_overweight`, `FAVC`, `SMOKE`, `SCC`), valid values `(\"yes\", \"no\")`, fill invalid/missing with mode.\n- **ordinal_categorical_cleaner**: For ordinal categorical features (`CAEC`, `CALC`), valid order `[\"no\", \"Sometimes\", \"Frequently\", \"Always\"]`, fill invalid/missing with mode or `\"no\"` if ambiguous.\n- **Manual harmonization**: For `MTRANS`, normalize string values and harmonize categories between train and test.\nExpected output or Impact on data: All features have correct types, no invalid values, and categories are consistent across train and test.\nConstraints: Do not drop or encode features at this stage; changes must be applied identically to train and test (except `NObeyesdad`, which is train-only).\n\n### STEP 2\nTask: Handle and impute missing values for all features.\nTools, involved features and correct parameters:\n- For numerical features, after type enforcement, use **impute_missing_by_group_median**:\n  - For `Height`, `Weight`, `Age`: group by `Gender` for imputation.\n  - For `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`: use overall median.\n- For categorical features, fill missing with mode or specified fill value in related cleaning tools.\nExpected output or Impact on data: No missing values in any feature in either train or test set.\nConstraints: Imputation statistics must be based on train data and applied identically to test; do not use target or future data for imputation.\n\n### STEP 3\nTask: Detect and treat outliers and implausible values in numerical features, especially BMI-related.\nTools, involved features and correct parameters:\n- **bmi_validator_and_flagger**: Calculate BMI from `Height` and `Weight`, flag implausible BMI (`BMI < 10` or `BMI > 80`), add `BMI` and `BMI_flag` columns.\n- For rows with `BMI_flag==True`:\n  - If BMI is just outside thresholds and height/weight plausible, use **cap_outliers** at 1st/99th percentiles.\n  - If BMI is far outside plausible range, drop record (train only) or cap/impute in test.\n- **iqr_outlier_flagger**: For each numerical feature, flag outliers (`iqr_mult=1.5`).\n- **cap_outliers**: Winsorize flagged numerical outliers at 1st/99th percentiles.\nExpected output or Impact on data: No implausible BMI or extreme outliers; distributions are robust and realistic.\nConstraints: Do not drop any rows from test; only cap or impute. Train set may drop records only if BMI is far outside plausible human range.\n\n### STEP 4\nTask: Harmonize categorical feature levels and handle rare categories.\nTools, involved features and correct parameters:\n- **handle_rare_categories**: For categorical features with >5 unique categories or rare entries (especially `MTRANS`, `CAEC`, `CALC`), set `threshold=10`, replace rare classes with `\"Rare\"`.\n- Check that all test categories exist in train; if not, map to `\"Rare\"`.\n- Harmonize spelling/capitalization of all categorical features between train and test.\nExpected output or Impact on data: All categorical features have consistent, clean, and manageable value sets across train and test.\nConstraints: Do not drop rows for rare categories; only relabel. No feature encoding or transformation at this phase.\n\nData Cleaning Report\n# FEATURE INFO\n## TARGET VARIABLE\nNObeyesdad\n## FEATURES BEFORE THIS PHASE\n['id', 'Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS', 'NObeyesdad']\n## FEATURES AFTER THIS PHASE\n['id', 'Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS', 'NObeyesdad', 'BMI', 'BMI_flag', 'Age_outlier_flag', 'Height_outlier_flag', 'Weight_outlier_flag', 'FCVC_outlier_flag', 'NCP_outlier_flag', 'CH2O_outlier_flag', 'FAF_outlier_flag', 'TUE_outlier_flag', 'BMI_outlier_flag']\n# REPORT\n\n## QUESTIONS AND ANSWERS  \n\n### Question 1\nWhat files did you process? Which files were generated? Answer with detailed file path.\n### Answer 1\nThe files processed in this phase are:\n- **Input files:**\n  - `train.csv`: Located at `/Users/scottiejj/Desktop/AutoKaggle_APAPTED/multi_agents/competition/obesity_risks/train.csv`\n  - `test.csv`: Located at `/Users/scottiejj/Desktop/AutoKaggle_APAPTED/multi_agents/competition/obesity_risks/test.csv`\n\n- **Output files generated:**\n  - `cleaned_train.csv`: Saved at `/Users/scottiejj/Desktop/AutoKaggle_APAPTED/multi_agents/competition/obesity_risks/cleaned_train.csv`\n  - `cleaned_test.csv`: Saved at `/Users/scottiejj/Desktop/AutoKaggle_APAPTED/multi_agents/competition/obesity_risks/cleaned_test.csv`\n\n---\n\n### Question 2\nWhich features were involved in this phase? What changes did they undergo? If any feature types were modified, answer which features are modified and how they are modified. If any features were deleted or created, answer which features are deleted or created and provide detailed explanations. \n### Answer 2\n**Features involved:**\n1. **Numerical Features:**\n   - `Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`\n     - **Changes:** Converted to float type, filled invalid values with the median computed from training data.\n  \n2. **Categorical Features:**\n   - `Gender`\n     - **Changes:** Filled invalid/missing values with the mode from the training set.\n   - `family_history_with_overweight`, `FAVC`, `SMOKE`, `SCC`\n     - **Changes:** Cleaned and filled invalid/missing values with the mode.\n   - `CAEC`, `CALC`\n     - **Changes:** Validated against an ordinal order, filled missing values with the mode or specified defaults.\n   - `MTRANS`\n     - **Changes:** Harmonized string values for consistency, ensuring valid categories match between train and test datasets.\n\n**Modified Features:**\n- All numerical features were modified to ensure correct data types and to handle invalid values.\n- Categorical features were modified to ensure consistency and valid values.\n\n**Deleted/Created Features:**\n- No features were deleted or created during this phase.\n\n---\n\n### Question 3\nWhat specific data cleaning steps were executed on each feature, and what were the outcomes?\n### Answer 3\n1. **Numerical Features:**\n   - Each numerical feature's invalid values were filled with the median computed from the training set, ensuring no NaNs remained.\n  \n2. **Gender:**\n   - Invalid entries were replaced with the mode (\"Female\"), ensuring all values are either \"Male\" or \"Female\".\n\n3. **Binary Categorical Features:**\n   - Invalid values for binary features were replaced with the mode, standardizing responses to \"yes\" or \"no\".\n\n4. **Ordinal Categorical Features:**\n   - Invalid values were replaced with the mode or \"no\" if ambiguous, ensuring all entries were valid within the specified order.\n\n5. **MTRANS:**\n   - String values were normalized (e.g., stripped, lowercased) for consistency, ensuring no discrepancies in categories between datasets.\n\n6. **Handling Missing Values:**\n   - Missing values in numerical features were filled using group median imputation based on gender and overall medians for others.\n   - Categorical features had missing values filled with the mode.\n\n7. **Outlier Handling:**\n   - Implausible BMI values were flagged, and extreme outliers were dropped from the train set or capped in the test set.\n   - Outliers for other numerical features were flagged using the IQR method and capped.\n\n---\n\n### Question 4\nWhich features had missing values before cleaning, and how were they addressed? Please provide the methods used for imputation and the final state of these features.\n### Answer 4\n- **Features with Missing Values:**\n  - `Age`, `Height`, `Weight`: Missing values were handled using group median imputation based on `Gender`.\n  - `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`: Filled with the overall median from the training set.\n\n**Final State:**\n- All features had missing values addressed, resulting in no NaNs remaining in the cleaned datasets.\n\n---\n\n### Question 5\nWhat outliers were identified in the dataset, and what strategies were implemented to handle them?\n### Answer 5\n- **Identified Outliers:**\n  - Outliers were identified for BMI values, where `BMI < 10` or `BMI > 80` were flagged as implausible.\n  - Additional outliers were flagged for all numerical features using the IQR method with a multiplier of 1.5.\n\n**Strategies Implemented:**\n- For extreme BMI values that were far outside the plausible range, records were dropped from the training set.\n- Capping was done for flagged outliers in both train and test sets at the 1st and 99th percentiles.\n\n---\n\n### Question 6\nWhat key insights or patterns emerged from the data cleaning phase that should be prioritized in the In-depth Exploratory Data Analysis?\n### Answer 6\n- The importance of maintaining consistency across categorical values, especially for features like `MTRANS`, suggests that harmonization will be crucial for accurate modeling.\n- The handling of missing values and outliers indicates that there may be underlying patterns related to demographics (e.g., gender differences in height and weight).\n- The imputation methods used highlight potential biases if certain groups have lower representation in the dataset, emphasizing the need to analyze these groups in more detail during the exploratory phase.\n- The identification of implausible BMI values suggests that further investigation into the relationships between height, weight, and other lifestyle factors may yield valuable insights into obesity risk factors.\n\n\n\n"
    },
    {
        "role": "assistant",
        "content": "```python\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Optional, Tuple, Dict, Union\nimport matplotlib\nmatplotlib.use('Agg')  # Prevents rendering during import\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency, f_oneway, kruskal, pearsonr, spearmanr\n\n\n# ---------------------------\n# Dataset-Specific EDA Tools\n# ---------------------------\n\ndef segment_bmi_by_obesity_class(\n    df: pd.DataFrame,\n    bmi_col: str,\n    target_col: str,\n    outlier_flag_col: Optional[str] = None\n) -> pd.DataFrame:\n    \"\"\"\n    Segment BMI statistics by obesity class.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        bmi_col (str): Name of the BMI column.\n        target_col (str): Name of the target column (obesity class).\n        outlier_flag_col (Optional[str]): Column indicating BMI outliers; if provided, outliers will be excluded.\n\n    Returns:\n        pd.DataFrame: BMI summary statistics (mean, std, min, max, median, count) for each obesity class.\n\n    Raises:\n        ValueError: If required columns are missing.\n\n    Return schema:\n        pd.DataFrame with columns:\n            - target_col (obesity class/category)\n            - mean\n            - std\n            - min\n            - max\n            - median\n            - count\n\n    Domain rationale:\n        Obesity class is directly related to BMI. This function quantifies BMI characteristics per class to reveal separation, overlaps, or labeling inconsistencies.\n\n    Example:\n        >>> segment_bmi_by_obesity_class(train, 'BMI', 'NObeyesdad')\n    \"\"\"\n    missing = [c for c in [bmi_col, target_col] if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    df_ = df.copy()\n    if outlier_flag_col:\n        if outlier_flag_col not in df.columns:\n            raise ValueError(f\"Outlier flag column '{outlier_flag_col}' not in dataframe.\")\n        df_ = df_[~df_[outlier_flag_col].astype(bool)]\n    stats = df_.groupby(target_col)[bmi_col].agg(['mean', 'std', 'min', 'max', 'median', 'count']).reset_index()\n    return stats\n\n\ndef target_stratified_numerical_summary(\n    df: pd.DataFrame,\n    numerical_cols: List[str],\n    target_col: str\n) -> pd.DataFrame:\n    \"\"\"\n    Compute summary statistics (mean, std, min, max, median) of numerical features stratified by target classes.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        numerical_cols (List[str]): List of numerical columns to summarize.\n        target_col (str): Name of the target column.\n\n    Returns:\n        pd.DataFrame: Multi-indexed summary statistics by target class.\n\n    Raises:\n        ValueError: If columns are missing.\n\n    Return schema:\n        pd.DataFrame with multi-index (target_col, stat), columns = numerical_cols\n\n    Domain rationale:\n        Obesity risk manifests differently across subgroups; stratified summaries reveal discriminative patterns and can guide feature engineering.\n\n    Example:\n        >>> target_stratified_numerical_summary(df, ['Age', 'Height', 'Weight'], 'NObeyesdad')\n    \"\"\"\n    missing = [c for c in [target_col] + numerical_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    grouped = df.groupby(target_col)[numerical_cols].agg(['mean', 'std', 'min', 'max', 'median', 'count'])\n    # Flatten the column MultiIndex\n    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n    grouped = grouped.reset_index()\n    return grouped\n\n\ndef categorical_vs_target_chi2(\n    df: pd.DataFrame,\n    cat_cols: List[str],\n    target_col: str\n) -> pd.DataFrame:\n    \"\"\"\n    Perform Chi-squared test between categorical features and the target variable.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        cat_cols (List[str]): List of categorical columns.\n        target_col (str): Name of the target column.\n\n    Returns:\n        pd.DataFrame: Results with columns ['feature', 'chi2', 'p_value', 'dof', 'significant'].\n\n    Raises:\n        ValueError: If columns are missing.\n\n    Return schema:\n        pd.DataFrame with columns:\n            - feature (str)\n            - chi2 (float)\n            - p_value (float)\n            - dof (int)\n            - significant (bool, p<0.05)\n\n    Domain rationale:\n        Categorical features (e.g. family history, food habits) are hypothesized to be linked to obesity risk. Chi2 tests quantify dependence strength.\n\n    Example:\n        >>> categorical_vs_target_chi2(df, ['Gender', 'FAVC'], 'NObeyesdad')\n    \"\"\"\n    missing = [c for c in [target_col] + cat_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    results = []\n    for col in cat_cols:\n        ct = pd.crosstab(df[col], df[target_col])\n        if ct.shape[0] < 2 or ct.shape[1] < 2:\n            results.append({'feature': col, 'chi2': np.nan, 'p_value': np.nan, 'dof': np.nan, 'significant': False})\n            continue\n        chi2, p, dof, _ = chi2_contingency(ct)\n        results.append({'feature': col, 'chi2': chi2, 'p_value': p, 'dof': dof, 'significant': p < 0.05})\n    return pd.DataFrame(results)\n\n\ndef target_vs_feature_anova(\n    df: pd.DataFrame,\n    numerical_cols: List[str],\n    target_col: str\n) -> pd.DataFrame:\n    \"\"\"\n    Perform ANOVA/Kruskal-Wallis test for each numerical feature across target classes.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        numerical_cols (List[str]): List of numerical columns.\n        target_col (str): Name of the target column.\n\n    Returns:\n        pd.DataFrame: Test results for each feature.\n\n    Raises:\n        ValueError: If columns are missing.\n\n    Return schema:\n        pd.DataFrame with columns:\n            - feature (str)\n            - test (str, 'anova' or 'kruskal')\n            - stat (float)\n            - p_value (float)\n            - significant (bool, p<0.05)\n\n    Domain rationale:\n        Quantifies whether means/medians of numerical features differ significantly between obesity classes.\n\n    Example:\n        >>> target_vs_feature_anova(df, ['Age', 'BMI'], 'NObeyesdad')\n    \"\"\"\n    missing = [c for c in [target_col] + numerical_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    results = []\n    for col in numerical_cols:\n        groups = [grp[1][col].dropna().values for grp in df.groupby(target_col)]\n        if any(len(arr) < 2 for arr in groups):\n            results.append({'feature': col, 'test': None, 'stat': np.nan, 'p_value': np.nan, 'significant': False})\n            continue\n        try:\n            stat, p = f_oneway(*groups)\n            test = 'anova'\n        except Exception:\n            stat, p = kruskal(*groups)\n            test = 'kruskal'\n        results.append({'feature': col, 'test': test, 'stat': stat, 'p_value': p, 'significant': p < 0.05})\n    return pd.DataFrame(results)\n\n\ndef feature_interaction_heatmap(\n    df: pd.DataFrame,\n    numerical_cols: List[str],\n    method: str = 'pearson',\n    figsize: Tuple[int, int] = (10, 8)\n) -> plt.Figure:\n    \"\"\"\n    Generate a heatmap of pairwise correlations (Pearson or Spearman) among numerical features.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        numerical_cols (List[str]): List of numerical columns to correlate.\n        method (str): Correlation method ('pearson' or 'spearman').\n        figsize (Tuple[int, int]): Figure size.\n\n    Returns:\n        matplotlib.figure.Figure: The generated heatmap figure.\n\n    Raises:\n        ValueError: If columns are missing or method invalid.\n\n    Return schema:\n        matplotlib.figure.Figure\n\n    Domain rationale:\n        Identifies which health/lifestyle features are interdependent, possibly confounded, or redundant for modeling.\n\n    Example:\n        >>> fig = feature_interaction_heatmap(df, ['Age', 'Weight', 'BMI'])\n    \"\"\"\n    if method not in {'pearson', 'spearman'}:\n        raise ValueError(\"method must be 'pearson' or 'spearman'\")\n    missing = [c for c in numerical_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n\n    corr = df[numerical_cols].corr(method=method)\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1, ax=ax)\n    ax.set_title(f'{method.capitalize()} Correlation Heatmap')\n    return fig\n\n\ndef lifestyle_segment_obesity_rate(\n    df: pd.DataFrame,\n    segment_col: str,\n    target_col: str,\n    min_count: int = 10\n) -> pd.DataFrame:\n    \"\"\"\n    Compute the proportion of each obesity level within groups defined by a lifestyle categorical feature.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        segment_col (str): Categorical feature for segmentation (e.g., 'FAVC', 'SMOKE', 'MTRANS').\n        target_col (str): Name of the target column.\n        min_count (int): Minimum group size to report; smaller groups are labeled 'Other'.\n\n    Returns:\n        pd.DataFrame: Obesity rate table (rows=segment categories, columns=target classes, values=proportions).\n\n    Raises:\n        ValueError: If columns are missing.\n\n    Return schema:\n        pd.DataFrame: index = segment_col, columns = sorted(target_col classes), values = proportion (float)\n\n    Domain rationale:\n        Lifestyle habits may stratify obesity risk; this table helps identify high-risk subgroups.\n\n    Example:\n        >>> lifestyle_segment_obesity_rate(df, 'SMOKE', 'NObeyesdad')\n    \"\"\"\n    missing = [c for c in [segment_col, target_col] if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    val_counts = df[segment_col].value_counts()\n    rare = val_counts[val_counts < min_count].index\n    df_segmented = df.copy()\n    df_segmented[segment_col] = df_segmented[segment_col].replace(rare, 'Other')\n    ctab = pd.crosstab(df_segmented[segment_col], df_segmented[target_col], normalize='index')\n    ctab = ctab.loc[:, sorted(ctab.columns)]\n    return ctab\n\n\n# ---------------------------\n# Generic/Reusable EDA Tools\n# ---------------------------\n\ndef summarize_numerical_features(\n    df: pd.DataFrame,\n    numerical_cols: List[str]\n) -> pd.DataFrame:\n    \"\"\"\n    Summarize basic statistics for numerical features.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        numerical_cols (List[str]): List of numerical columns.\n\n    Returns:\n        pd.DataFrame: Table of statistics (mean, std, min, max, median, count, n_unique, n_missing).\n\n    Raises:\n        ValueError: If columns are missing.\n\n    Return schema:\n        pd.DataFrame with columns:\n            - feature (str)\n            - mean, std, min, max, median, count, n_unique, n_missing\n\n    Example:\n        >>> summarize_numerical_features(df, ['Age', 'BMI'])\n    \"\"\"\n    missing = [c for c in numerical_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    summaries = []\n    for col in numerical_cols:\n        s = df[col]\n        summaries.append({\n            'feature': col,\n            'mean': s.mean(),\n            'std': s.std(),\n            'min': s.min(),\n            'max': s.max(),\n            'median': s.median(),\n            'count': s.count(),\n            'n_unique': s.nunique(),\n            'n_missing': s.isnull().sum()\n        })\n    return pd.DataFrame(summaries)\n\n\ndef summarize_categorical_features(\n    df: pd.DataFrame,\n    categorical_cols: List[str]\n) -> pd.DataFrame:\n    \"\"\"\n    Summarize counts, unique values, and missingness for categorical features.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        categorical_cols (List[str]): List of categorical columns.\n\n    Returns:\n        pd.DataFrame: Table with feature, n_unique, top (mode), top_count, n_missing.\n\n    Raises:\n        ValueError: If columns are missing.\n\n    Return schema:\n        pd.DataFrame with columns:\n            - feature (str)\n            - n_unique (int)\n            - top (object)\n            - top_count (int)\n            - n_missing (int)\n\n    Example:\n        >>> summarize_categorical_features(df, ['Gender', 'FAVC'])\n    \"\"\"\n    missing = [c for c in categorical_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    summaries = []\n    for col in categorical_cols:\n        s = df[col]\n        top = s.mode().iloc[0] if not s.mode().empty else np.nan\n        top_count = s.value_counts().iloc[0] if not s.value_counts().empty else 0\n        summaries.append({\n            'feature': col,\n            'n_unique': s.nunique(),\n            'top': top,\n            'top_count': top_count,\n            'n_missing': s.isnull().sum()\n        })\n    return pd.DataFrame(summaries)\n\n\ndef automated_outlier_detector(\n    df: pd.DataFrame,\n    numerical_cols: List[str],\n    method: str = 'iqr',\n    threshold: float = 1.5\n) -> pd.DataFrame:\n    \"\"\"\n    Detect outliers in numerical columns using the IQR or Z-score method.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        numerical_cols (List[str]): List of numerical columns.\n        method (str): 'iqr' or 'zscore'.\n        threshold (float): Threshold multiplier for outlier detection.\n\n    Returns:\n        pd.DataFrame: Table with feature, n_outliers, outlier_ratio.\n\n    Raises:\n        ValueError: If columns are missing or method invalid.\n\n    Return schema:\n        pd.DataFrame with columns:\n            - feature (str)\n            - n_outliers (int)\n            - outlier_ratio (float)\n\n    Example:\n        >>> automated_outlier_detector(df, ['BMI'], method='iqr', threshold=1.5)\n    \"\"\"\n    if method not in {'iqr', 'zscore'}:\n        raise ValueError(\"method must be 'iqr' or 'zscore'\")\n    missing = [c for c in numerical_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    results = []\n    for col in numerical_cols:\n        s = df[col].dropna()\n        if s.empty:\n            results.append({'feature': col, 'n_outliers': 0, 'outlier_ratio': 0.0})\n            continue\n        if method == 'iqr':\n            q1, q3 = s.quantile([0.25, 0.75])\n            iqr = q3 - q1\n            lower = q1 - threshold * iqr\n            upper = q3 + threshold * iqr\n            mask = (s < lower) | (s > upper)\n        else:  # zscore\n            mean = s.mean()\n            std = s.std()\n            if std == 0:\n                mask = pd.Series([False]*len(s), index=s.index)\n            else:\n                mask = np.abs((s - mean) / std) > threshold\n        n_outliers = int(mask.sum())\n        outlier_ratio = float(n_outliers) / len(s)\n        results.append({'feature': col, 'n_outliers': n_outliers, 'outlier_ratio': outlier_ratio})\n    return pd.DataFrame(results)\n\n\ndef plot_numerical_distributions(\n    df: pd.DataFrame,\n    numerical_cols: List[str],\n    n_cols: int = 3,\n    figsize: Tuple[int, int] = (16, 8)\n) -> plt.Figure:\n    \"\"\"\n    Create histograms for all numerical features.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        numerical_cols (List[str]): List of numerical columns.\n        n_cols (int): Number of columns in subplot grid.\n        figsize (Tuple[int, int]): Figure size.\n\n    Returns:\n        matplotlib.figure.Figure: The generated figure.\n\n    Raises:\n        ValueError: If columns are missing.\n\n    Return schema:\n        matplotlib.figure.Figure\n\n    Example:\n        >>> fig = plot_numerical_distributions(df, ['Age', 'Weight'])\n    \"\"\"\n    missing = [c for c in numerical_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    n_plots = len(numerical_cols)\n    n_rows = int(np.ceil(n_plots / n_cols))\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    axes = np.array(axes).reshape(-1)\n    for idx, col in enumerate(numerical_cols):\n        ax = axes[idx]\n        sns.histplot(df[col].dropna(), kde=True, ax=ax, bins=30, color='skyblue')\n        ax.set_title(col)\n    for ax in axes[n_plots:]:\n        ax.axis('off')\n    fig.tight_layout()\n    return fig\n\n\ndef plot_categorical_counts(\n    df: pd.DataFrame,\n    categorical_cols: List[str],\n    n_cols: int = 3,\n    figsize: Tuple[int, int] = (16, 8)\n) -> plt.Figure:\n    \"\"\"\n    Create countplots for all categorical features.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        categorical_cols (List[str]): List of categorical columns.\n        n_cols (int): Number of columns in subplot grid.\n        figsize (Tuple[int, int]): Figure size.\n\n    Returns:\n        matplotlib.figure.Figure: The generated figure.\n\n    Raises:\n        ValueError: If columns are missing.\n\n    Return schema:\n        matplotlib.figure.Figure\n\n    Example:\n        >>> fig = plot_categorical_counts(df, ['Gender', 'FAVC'])\n    \"\"\"\n    missing = [c for c in categorical_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in dataframe: {missing}\")\n    n_plots = len(categorical_cols)\n    n_rows = int(np.ceil(n_plots / n_cols))\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    axes = np.array(axes).reshape(-1)\n    for idx, col in enumerate(categorical_cols):\n        ax = axes[idx]\n        sns.countplot(x=col, data=df, ax=ax, order=sorted(df[col].dropna().unique()))\n        ax.set_title(col)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n    for ax in axes[n_plots:]:\n        ax.axis('off')\n    fig.tight_layout()\n    return fig\n\n\ndef feature_vs_target_boxplot(\n    df: pd.DataFrame,\n    numerical_col: str,\n    target_col: str,\n    figsize: Tuple[int, int] = (8, 5)\n) -> plt.Figure:\n    \"\"\"\n    Create a boxplot of a numerical feature grouped by target class.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        numerical_col (str): Numerical column to plot.\n        target_col (str): Target/categorical column for grouping.\n        figsize (Tuple[int, int]): Figure size.\n\n    Returns:\n        matplotlib.figure.Figure: The generated figure.\n\n    Raises:\n        ValueError: If columns are missing.\n\n    Return schema:\n        matplotlib.figure.Figure\n\n    Example:\n        >>> fig = feature_vs_target_boxplot(df, 'BMI', 'NObeyesdad')\n    \"\"\"\n    if numerical_col not in df.columns or target_col not in df.columns:\n        raise ValueError(f\"Missing columns: {[c for c in [numerical_col, target_col] if c not in df.columns]}\")\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.boxplot(x=target_col, y=numerical_col, data=df, ax=ax, order=sorted(df[target_col].dropna().unique()))\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n    ax.set_title(f\"{numerical_col} by {target_col}\")\n    return fig\n\n\ndef analyze_target_distribution(\n    df: pd.DataFrame,\n    target_col: str\n) -> pd.DataFrame:\n    \"\"\"\n    Summarize counts and proportions of each class in the target variable.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        target_col (str): Name of the target column.\n\n    Returns:\n        pd.DataFrame: Table with class, count, and proportion.\n\n    Raises:\n        ValueError: If column is missing.\n\n    Return schema:\n        pd.DataFrame with columns:\n            - class (object)\n            - count (int)\n            - proportion (float)\n\n    Example:\n        >>> analyze_target_distribution(df, 'NObeyesdad')\n    \"\"\"\n    if target_col not in df.columns:\n        raise ValueError(f\"Missing target column: {target_col}\")\n    counts = df[target_col].value_counts(dropna=False)\n    proportions = counts / counts.sum()\n    result = pd.DataFrame({'class': counts.index, 'count': counts.values, 'proportion': proportions.values})\n    return result\n\n```"
    },
    {
        "role": "user",
        "content": "\n## TASK: CONVERT PYTHON TOOLS TO MARKDOWN DOCS ##\n\nYou will receive a complete Python module containing multiple tool functions.\n\nRules:\n- Use the function name as the tool name\n- Derive parameter info and types from the function signature and docstring\n- Keep the formatting EXACT (labels, backticks, enums, separators) per the spec\n- Output ONLY one concatenated fenced block: ```markdown\\n...sections...\\n```\n- Do not include any prose before or after the fenced block.\n- If a function is dataset-specific, include a note line in the Notes section starting with \"- Domain rationale:\" that briefly explains why the tool is suited to this dataset/competition\n\n\nFor EACH top-level function, produce a markdown section that follows the EXACT format of the example below.\n\nHere is the MARKDOWN format you should follow (example):\n```markdown\n## fill_missing_values\n\n**Name:** fill_missing_values  \n**Description:** Fill missing values in specified columns of a DataFrame. This tool can handle both numerical and categorical features by using different filling methods.  \n**Applicable Situations:** handle missing values in various types of features\n\n**Parameters:**\n- `data`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** A pandas DataFrame object representing the dataset.\n- `columns`:\n  - **Type:** ``string` | `array``\n  - **Description:** The name(s) of the column(s) where missing values should be filled.\n- `method`:\n  - **Type:** `string`\n  - **Description:** The method to use for filling missing values.\n  - **Enum:** `auto` | `mean` | `median` | `mode` | `constant`\n  - **Default:** `auto`\n- `fill_value`:\n  - **Type:** ``number` | `string` | `null``\n  - **Description:** The value to use when method is 'constant'.\n  - **Default:** `None`\n\n**Required:** `data`, `columns`  \n**Result:** Successfully fill missing values in the specified column(s) of data  \n**Notes:**\n- The 'auto' method uses mean for numeric columns and mode for non-numeric columns.\n- Using 'mean' or 'median' on non-numeric columns will raise an error.\n- The 'mode' method uses the most frequent value, which may not always be appropriate.\n- Filling missing values can introduce bias, especially if the data is not missing completely at random.\n- Consider the impact of filling missing values on your analysis and model performance.\n\n---\n```\n\nPYTHON MODULE SOURCE:\n{python_source}\n"
    },
    {
        "role": "assistant",
        "content": "```markdown\n## segment_bmi_by_obesity_class\n\n**Name:** segment_bmi_by_obesity_class  \n**Description:** Segment BMI statistics by obesity class. Produces summary statistics (mean, std, min, max, median, count) of BMI for each obesity class, optionally excluding BMI outliers.  \n**Applicable Situations:** Analyze BMI distribution across target obesity classes, verify class separability or labeling consistency.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe containing BMI, target, and (optionally) outlier flag columns.\n- `bmi_col`:\n  - **Type:** `string`\n  - **Description:** Name of the BMI column.\n- `target_col`:\n  - **Type:** `string`\n  - **Description:** Name of the target column (obesity class).\n- `outlier_flag_col`:\n  - **Type:** `string | None`\n  - **Description:** Optional column indicating BMI outliers; if provided, outliers will be excluded.\n  - **Default:** `None`\n\n**Required:** `df`, `bmi_col`, `target_col`  \n**Result:** DataFrame with BMI summary statistics for each obesity class  \n**Notes:**\n- Raises `ValueError` if required columns are missing.\n- - Domain rationale: Obesity level is directly related to BMI; this tool quantifies BMI characteristics per class to reveal separation, overlaps, or labeling inconsistencies.\n\n---\n\n## target_stratified_numerical_summary\n\n**Name:** target_stratified_numerical_summary  \n**Description:** Compute summary statistics (mean, std, min, max, median, count) of numerical features stratified by target classes.  \n**Applicable Situations:** Explore how health/lifestyle features differ across obesity groups.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `numerical_cols`:\n  - **Type:** `array`\n  - **Description:** List of numerical columns to summarize.\n- `target_col`:\n  - **Type:** `string`\n  - **Description:** Name of the target column.\n\n**Required:** `df`, `numerical_cols`, `target_col`  \n**Result:** DataFrame with multi-column summary statistics for each target class  \n**Notes:**\n- Raises `ValueError` if any specified columns are missing.\n- - Domain rationale: Obesity risk manifests differently across subgroups; stratified summaries reveal discriminative patterns and can guide feature engineering.\n\n---\n\n## categorical_vs_target_chi2\n\n**Name:** categorical_vs_target_chi2  \n**Description:** Perform Chi-squared test between categorical features and the target variable to assess association strength.  \n**Applicable Situations:** Identify which categorical features are statistically linked to obesity class.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `cat_cols`:\n  - **Type:** `array`\n  - **Description:** List of categorical columns.\n- `target_col`:\n  - **Type:** `string`\n  - **Description:** Name of the target column.\n\n**Required:** `df`, `cat_cols`, `target_col`  \n**Result:** DataFrame with columns [`feature`, `chi2`, `p_value`, `dof`, `significant`]  \n**Notes:**\n- Returns NaN for features with insufficient classes.\n- Raises `ValueError` if required columns are missing.\n- - Domain rationale: Categorical features (e.g. family history, food habits) are hypothesized to be linked to obesity risk. Chi2 tests quantify dependence strength.\n\n---\n\n## target_vs_feature_anova\n\n**Name:** target_vs_feature_anova  \n**Description:** Perform ANOVA (or fallback to Kruskal-Wallis) for each numerical feature across target classes to test for group differences.  \n**Applicable Situations:** Quantify whether feature means/medians differ significantly between obesity classes.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `numerical_cols`:\n  - **Type:** `array`\n  - **Description:** List of numerical columns.\n- `target_col`:\n  - **Type:** `string`\n  - **Description:** Name of the target column.\n\n**Required:** `df`, `numerical_cols`, `target_col`  \n**Result:** DataFrame with columns [`feature`, `test`, `stat`, `p_value`, `significant`]  \n**Notes:**\n- Uses ANOVA if possible, falls back to Kruskal-Wallis for non-normal distributions or errors.\n- Returns NaN if groups too small for test.\n- Raises `ValueError` if columns are missing.\n- - Domain rationale: Quantifies whether means/medians of numerical features differ significantly between obesity classes.\n\n---\n\n## feature_interaction_heatmap\n\n**Name:** feature_interaction_heatmap  \n**Description:** Generate a heatmap of pairwise correlations (Pearson or Spearman) among numerical features.  \n**Applicable Situations:** Visualize feature dependencies, multicollinearity, or redundancy.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `numerical_cols`:\n  - **Type:** `array`\n  - **Description:** List of numerical columns to correlate.\n- `method`:\n  - **Type:** `string`\n  - **Description:** Correlation method.\n  - **Enum:** `pearson` | `spearman`\n  - **Default:** `pearson`\n- `figsize`:\n  - **Type:** `tuple`\n  - **Description:** Figure size for the heatmap.\n  - **Default:** `(10, 8)`\n\n**Required:** `df`, `numerical_cols`  \n**Result:** `matplotlib.figure.Figure` heatmap object  \n**Notes:**\n- Raises `ValueError` if columns missing or method invalid.\n- - Domain rationale: Identifies which health/lifestyle features are interdependent, possibly confounded, or redundant for modeling.\n\n---\n\n## lifestyle_segment_obesity_rate\n\n**Name:** lifestyle_segment_obesity_rate  \n**Description:** Compute the proportion of each obesity level within groups defined by a lifestyle categorical feature.  \n**Applicable Situations:** Identify high-risk or protective lifestyle subgroups.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `segment_col`:\n  - **Type:** `string`\n  - **Description:** Categorical feature for segmentation (e.g., 'FAVC', 'SMOKE', 'MTRANS').\n- `target_col`:\n  - **Type:** `string`\n  - **Description:** Name of the target column.\n- `min_count`:\n  - **Type:** `int`\n  - **Description:** Minimum group size to report; smaller groups are labeled 'Other'.\n  - **Default:** `10`\n\n**Required:** `df`, `segment_col`, `target_col`  \n**Result:** DataFrame with rows=segment categories, columns=obesity classes, values=proportions  \n**Notes:**\n- Groups smaller than `min_count` are labeled 'Other' to avoid misleading rates.\n- Raises `ValueError` if columns are missing.\n- - Domain rationale: Lifestyle habits may stratify obesity risk; this table helps identify high-risk subgroups.\n\n---\n\n## summarize_numerical_features\n\n**Name:** summarize_numerical_features  \n**Description:** Summarize basic statistics for numerical features, including mean, std, min, max, median, count, n_unique, n_missing.  \n**Applicable Situations:** Get overall sense of numerical data distribution, spot potential anomalies.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `numerical_cols`:\n  - **Type:** `array`\n  - **Description:** List of numerical columns.\n\n**Required:** `df`, `numerical_cols`  \n**Result:** DataFrame of statistics for each feature  \n**Notes:**\n- Raises `ValueError` if specified columns are missing.\n\n---\n\n## summarize_categorical_features\n\n**Name:** summarize_categorical_features  \n**Description:** Summarize counts, unique values, mode, and missingness for categorical features.  \n**Applicable Situations:** Audit categorical feature cardinality and completeness.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `categorical_cols`:\n  - **Type:** `array`\n  - **Description:** List of categorical columns.\n\n**Required:** `df`, `categorical_cols`  \n**Result:** DataFrame with summary info for each feature  \n**Notes:**\n- Raises `ValueError` if specified columns are missing.\n\n---\n\n## automated_outlier_detector\n\n**Name:** automated_outlier_detector  \n**Description:** Detect outliers in numerical columns using the IQR or Z-score method. Returns count and ratio of outliers per feature.  \n**Applicable Situations:** Quickly identify features with potential outlier issues.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `numerical_cols`:\n  - **Type:** `array`\n  - **Description:** List of numerical columns.\n- `method`:\n  - **Type:** `string`\n  - **Description:** Outlier detection method.\n  - **Enum:** `iqr` | `zscore`\n  - **Default:** `iqr`\n- `threshold`:\n  - **Type:** `float`\n  - **Description:** Threshold multiplier for outlier detection.\n  - **Default:** `1.5`\n\n**Required:** `df`, `numerical_cols`  \n**Result:** DataFrame with outlier count and ratio per feature  \n**Notes:**\n- Raises `ValueError` if columns are missing or method is invalid.\n\n---\n\n## plot_numerical_distributions\n\n**Name:** plot_numerical_distributions  \n**Description:** Create histograms for all numerical features, with density overlay.  \n**Applicable Situations:** Visualize distributions and spot skew, multimodality, or outliers.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `numerical_cols`:\n  - **Type:** `array`\n  - **Description:** List of numerical columns.\n- `n_cols`:\n  - **Type:** `int`\n  - **Description:** Number of subplot columns.\n  - **Default:** `3`\n- `figsize`:\n  - **Type:** `tuple`\n  - **Description:** Figure size.\n  - **Default:** `(16, 8)`\n\n**Required:** `df`, `numerical_cols`  \n**Result:** `matplotlib.figure.Figure` with subplots  \n**Notes:**\n- Raises `ValueError` if columns are missing.\n- Returns a matplotlib Figure; does not display or save.\n\n---\n\n## plot_categorical_counts\n\n**Name:** plot_categorical_counts  \n**Description:** Create countplots for all categorical features in a grid.  \n**Applicable Situations:** Visualize commonality and balance of categorical feature values.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `categorical_cols`:\n  - **Type:** `array`\n  - **Description:** List of categorical columns.\n- `n_cols`:\n  - **Type:** `int`\n  - **Description:** Number of subplot columns.\n  - **Default:** `3`\n- `figsize`:\n  - **Type:** `tuple`\n  - **Description:** Figure size.\n  - **Default:** `(16, 8)`\n\n**Required:** `df`, `categorical_cols`  \n**Result:** `matplotlib.figure.Figure` with subplots  \n**Notes:**\n- Raises `ValueError` if columns are missing.\n- Returns a matplotlib Figure; does not display or save.\n\n---\n\n## feature_vs_target_boxplot\n\n**Name:** feature_vs_target_boxplot  \n**Description:** Create a boxplot of a numerical feature grouped by target class.  \n**Applicable Situations:** Visualize central tendency and spread of a feature across obesity groups.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `numerical_col`:\n  - **Type:** `string`\n  - **Description:** Numerical column to plot.\n- `target_col`:\n  - **Type:** `string`\n  - **Description:** Target/categorical column for grouping.\n- `figsize`:\n  - **Type:** `tuple`\n  - **Description:** Figure size.\n  - **Default:** `(8, 5)`\n\n**Required:** `df`, `numerical_col`, `target_col`  \n**Result:** `matplotlib.figure.Figure` for boxplot  \n**Notes:**\n- Raises `ValueError` if columns are missing.\n- Returns a matplotlib Figure; does not display or save.\n\n---\n\n## analyze_target_distribution\n\n**Name:** analyze_target_distribution  \n**Description:** Summarize counts and proportions of each class in the target variable.  \n**Applicable Situations:** Assess class balance and rarity for classification tasks.\n\n**Parameters:**\n- `df`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** Input dataframe.\n- `target_col`:\n  - **Type:** `string`\n  - **Description:** Name of the target column.\n\n**Required:** `df`, `target_col`  \n**Result:** DataFrame with class, count, and proportion  \n**Notes:**\n- Raises `ValueError` if target column is missing.\n\n---\n```"
    }
]